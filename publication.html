<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Publications</title>
	<meta content="Publications, wanglimin.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong, b {
	font-weight:bold;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wanglimin.github.io');
  ga('send', 'pageview');

</script>
<body>

<div style="clear: both;">
<div class="section">
  <h2>Technical Reports</h2>
  <div class="paper">
    <ul>
  <li>
   <strong>L. Wang</strong>, S. Guo, W. Huang, and Y. Qiao <br />
   Places205-VGGNet Models for Scene Recognition <br />
   in arXiv 1508.01667. 
  </li>    
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Journal Papers</h2>
  <div class="paper">
    <ul>

    <li> 
    Z. Yuan, H. Wang, <strong>L. Wang</strong>, T. Lu, P. Shivakumara, and C. L. Tan<br />
    <a href='papers/YuanWWL_ESWA16.pdf'><strong>Modeling Spatial Layout for Scene Image Understanding via a Novel Multiscale Sum-Product Network</strong></a> <br />
    in Expert Systems With Applications (<strong>ESWA</strong>), Volume 63, Page 231-240, 2016. (impact factor: 2.981) <br />
    [ <a href='papers/YuanWWL_ESWA16.pdf'>Paper</a> ]  [ <a href='papers/YuanWWL_ESWA16.bib'>BibTex</a> ]
    </li> 

    <li> 
    X. Peng, <strong>L. Wang</strong>, X. Wang, and Y. Qiao <br />
    <a href='papers/PengWWQ_CVIU16.pdf'><strong>Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice</strong></a> <br />
    in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 150, Page 109-125, 2016. (impact factor: 2.134)<br />
    [ <a href='papers/PengWWQ_CVIU16.pdf'>Paper</a> ]  [ <a href='papers/PengWWQ_CVIU16.bib'>BibTex</a> ]
    </li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_IJCV15.pdf'><strong>MoFAP: A Multi-Level Representation for Action Recognition</strong></a> <br />
    in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 119, Number 3, Page 254-271, 2016. (impact factor: 4.270)<br />
    [ <a href='papers/WangQT_IJCV15.pdf'>Paper</a> ]  [ <a href='papers/WangQT_IJCV15.bib'>BibTex</a> ]
		</li>

	  <li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_TIP14.pdf'><strong>Latent Hierarchical Model of Temporal Structure for Complex Activity Classification</strong></a> <br />
    in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 23, Number 2, Page 810-822, 2014. (impact factor: 3.735)<br />
    [ <a href='papers/WangQT_TIP14.pdf'>Paper</a> ]  [ <a href='papers/WangQT_TIP14.bib'>BibTex</a> ]
		</li>

    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>CVPR/ICCV/ECCV Papers</h2>
  <div class="paper">
    <ul>

    <li> 
    <strong>L. Wang</strong>, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool <br />
    <a href=''><strong>Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</strong></a><br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2016. <br />
    [ <a href=''>Paper</a> ]  [ <a href=''>BibTex</a> ] [ <a href=''>Poster</a> ]
    </li>
    

    <li> 
    <strong>L. Wang</strong>, Y. Qiao, X. Tang, and L. Van Gool <br />
    <a href='papers/WangQTV_CVPR16.pdf'><strong>Actionness Estimation Using Hybrid Fully Convolutional Networks</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br />
    [ <a href='papers/WangQTV_CVPR16.pdf'>Paper</a> ]  [ <a href='papers/WangQTV_CVPR16.bib'>BibTex</a> ] [ <a href='papers/WangQTV_CVPR16_Poster.pdf'>Poster</a> ] [ <a href = 'actionness/index.html'>Project Page</a> ] [ <a href='https://github.com/wanglimin/actionness-estimation/'>Code</a> ]
    </li>


    <li> 
    B. Zhang, <strong>L. Wang</strong>, Z. Wang, Y. Qiao, and H. Wang <br />
    <a href='papers/ZhangWWQW_CVPR16.pdf'><strong>Real-time Action Recognition with Enhanced Motion Vector CNNs</strong></a><br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br />
    [ <a href='papers/ZhangWWQW_CVPR16.pdf'>Paper</a> ]  [ <a href='papers/ZhangWWQW_CVPR16.bib'>BibTex</a> ] [ <a href='papers/ZhangWWQW_CVPR16_Poster.pdf'>Poster</a> ] [ <a href = 'http://zbwglory.github.io/MV-CNN/index.html'>Project Page</a> ] [ <a href="https://github.com/zbwglory/MV-release">Code</a> ]
    </li>

    <li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_CVPR15.pdf'><strong>Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2015. <br />
    [ <a href='papers/WangQT_CVPR15.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR15.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR15_Poster.pdf'>Poster<a> ] [ <a href='papers/WangQT_CVPR15_abstract.pdf'>Extended Abstract</a> ] [ <a href = 'tdd/index.html'>Project Page</a> ] [ <a href = 'https://github.com/wanglimin/TDD'>Code</a> ] 
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_ECCV14.pdf'><strong>Video Action Detection with Relational Dynamic-Poselets</strong></a> <br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2014. <br />
		[ <a href='papers/WangQT_ECCV14.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ECCV14.bib'>BibTex</a> ] [ <a href='papers/WangQT_ECCV14_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ECCV14_Spotlight.wmv'>Spotlight</a> ] [ <a href='release_plot.rar'>Code</a> ]
		</li>
    <li> 
    Z. Cai, <strong>L. Wang</strong>, X. Peng, and Y. Qiao <br />
    <a href='papers/CaiWPQ_CVPR14.pdf'><strong>Multi-View Super Vector for Action Recognition</strong></a> ( <alert>oral presentation</alert> ) <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2014. <br />
    [ <a href='papers/CaiWPQ_CVPR14.pdf'>Paper</a> ]  [ <a href='papers/CaiWPQ_CVPR14.bib'>BibTex</a> ] [ <a href='papers/CaiWPQ_CVPR14.mp4'>Video Spotlight</a> ] [ <a href='papers/CaiWPQ_CVPR14_oral.ppt'>Oral Presentation</a> ] [ <a href='papers/CaiWPQ_CVPR14_poster.ppt'>Poster</a> ] [ <a href='papers/CaiWPQ_CVPR14_Supplement.pdf'>Supplement</a> ] [ <a href='papers/CaiWPQ_CVPR14.zip'>Code</a> ]  
    </li>
    <li> 
    X. Peng*, <strong>L. Wang</strong>*, Y. Qiao, and Q. Peng <br />
    <a href='papers/PengWQP_ECCV14.pdf'><strong>Boosting VLAD with Supervised Dictionary Learning and High-Order Statistics</strong></a> <br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2014. <br />
    [ <a href='papers/PengWQP_ECCV14.pdf'>Paper</a> ] [ <a href='papers/PengWQP_ECCV14.bib'>BibTex</a> ] 
    </li>
		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_ICCV13.pdf'><strong>Mining Motion Atoms and Phrases for Complex Action Recognition</strong></a> <br />
     in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2013. <br />
		[ <a href='papers/WangQT_ICCV13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ICCV13.bib'>BibTex</a> ] [ <a href='papers/WangQT_ICCV13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ICCV13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'mofap/index.html'>Project Page</a> ] 
		</li>
		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_CVPR13.pdf'><strong>Motionlets: Mid-Level 3D Parts for Human Motion Recognition</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2013. <br />
		[ <a href='papers/WangQT_CVPR13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR13.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_CVPR13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'motionlet/index.html'>Project Page</a> ]
		</li>
		
	</ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Other Conference Papers</h2>
  <div class="paper">
    <ul>

    <li>
    Y. Wang, J. Song, <strong>L. Wang</strong>, O. Hilliges, and L. Van Gool<br />
    Two-Stream SR-CNNs for Action Recognition in Videos <br />
    in British Machine Vision Conference 2016 (<strong>BMVC</strong>), 2016. <br />
    [ <a href=''>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li> 

    <li>
    Z. Wang, Y. Wang, <strong>L. Wang</strong>, and Y. Qiao <br />
    Codebook Enhancement of VLAD Representation for Visual Recognition <br />
    in IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2016. <br />
    [ <a href='papers/WangWWQ_ICASSP16.pdf'>Paper</a> ] [ <a href='papers/WangWWQ_ICASSP16.bib'>BibTex</a> ] 
    </li> 

		<li> 
		X. Peng, <strong>L. Wang</strong>, Y. Qiao, and Q. Peng <br />
    A Joint Evaluation of Dictionary Learning and Feature Encoding for Action Recognition <br />
    in International Conference on Pattern Recognition (<strong>ICPR</strong>), 2014. <br />
    [ <a href='papers/PengWQP_ICPR14.pdf'>Paper</a> ] [ <a href='papers/PengWQP_ICPR14.bib'>BibTex</a> ] 
		</li>

		<li> 
		X. Wang, <strong>L. Wang</strong>, and Y. Qiao <br />
    A Comparative Study of Encoding, Pooling and Normalization Methods for Action Recognition <br />
    in Asian Conference on Computer Vision (<strong>ACCV</strong>), 2012. <br />
    [ <a href='papers/WangWQ_ACCV12.pdf'>Paper</a> ] [ <a href='papers/WangWQ_ACCV12.bib'>BibTex</a> ] [ <a href='papers/WangWQ_ACCV12_Poster.pdf'>Poster</a> ] [ <a href='papers/WangWQ_ACCV12_Spotlight.ppt'>Spotlight</a> ]
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Wu, T. Lu, and K. Chen <br />
    Multiclass Object Detection by Combining Local Appearances and Context <br />
    in ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2011. <br />
    [ <a href='papers/WangWLC_MM11.pdf'>Paper</a> ] [ <a href='papers/WangWLC_MM11.bib'>BibTex</a> ]
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Wu, Z. Tian, Z. Sun, and T. Lu <br />
    A Novel Approach for Robust Surveillance Video Content Abstraction <br />
    in Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2010. <br />
    [ <a href='papers/WangWTSL_PCM10.pdf'>Paper</a> ] [ <a href='papers/WangWTSL_PCM10.bib'>BibTex</a> ]
		</li>

    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Workshop and Notebook Papers</h2>
  <div class="paper">
    <ul>

    <li>
      Y. Xiong, <strong>L. Wang</strong>, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao, L. Van Gool, and X. Tang <br />
      CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 ( <alert>rank 1st place</alert> ) <br />
      in ActivityNet Large Scale Activity Recognition Challenge, <strong>CVPR</strong>, 2016. <br />
      [ <a href='contests/XiongW_ActivityNet16.pdf'>Paper</a> ] [ BibTex] [ Presentation ] <br /> 
    </li>

    <li>
    <strong>L. Wang</strong>, Z. Wang, S. Guo, and Y. Qiao <br />
    Better Exploiting OS-CNNs for Better Event Recognition in Images <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>ICCV</strong>, 2015. <br />
    [ <a href='contests/WangWGQ_ChaLearnLAP15.pdf'>Paper</a>  ] [ <a href='contests/WangWGQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='contests/WangWGQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ]
    </li>

		<li>
		<strong>L. Wang</strong>, Z. Wang, Y. Xiong, and Y. Qiao <br />
    CUHK&SIAT Submission for THUMOS15 Action Recognition Challenge <br /> 
    in <strong>THUMOS'15</strong> Action Recognition Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWXQ_THUMOS15.pdf'>Paper</a> ] [ <a href='contests/WangWXQ_Thumos15.bib'>BibTex</a> ] [ <a href='contests/WangWXQ_Thumos15_slide.pdf'>Presentation</a> ] <br /> 
		</li>

		<li>
		<strong>L. Wang</strong>, Z. Wang, W. Du, and Y. Qiao <br />
    Object-Scene Convolutional Neural Networks for Event Recognition in Images ( <alert>rank 1st place</alert> )<br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWDQ_ChaLearnLAP15.pdf'>Paper</a>  ]  [ <a href='contests/WangWDQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='contests/WangWDQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ] <br /> 
		</li>

		<li>
		Z. Wang, <strong>L. Wang</strong>, W. Du, and Y. Qiao <br />
    Exploring Fisher Vector and Deep Networks for Action Spotting ( <alert>rank 1st place</alert> ) <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWDQ_LAP15.pdf'>Paper</a>  ]  [ <a href='contests/WangWDQ_LAP15.bib'>BibTex</a> ] [ <a href='contests/WangWDQ_LAP15_slide.pdf'>Presentation</a> ] <br /> 
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    Action Recognition and Detection by Combining Motion and Appearance Features, <br />
    in <strong>THUMOS'14</strong> Action Recognition Challenge, <strong>ECCV</strong>, 2014. <br />
    [ <a href='contests/WangQT_Thumos14.pdf'>Paper</a> ] [ <a href='contests/WangQT_Thumos14.bib'>BibTex</a> ]  [ <a href='contests/WangQT_Thumos14_slide.pdf'>Presentation</a> ] 
		</li>

		<li> 
		X. Peng, <strong>L. Wang</strong>, Z. Cai, and Y. Qiao <br />
    Action and Gesture Temporal Spotting with Super Vector Representation ( <alert>rank 1st place</alert> ) <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>ECCV</strong>, 2014. <br />
    [ <a href='contests/PengWCQ_LAP14.pdf'>Paper</a> ] [ <a href='contests/PengWCQ_LAP14.bib'>BibTex</a> ]  [ <a href='contests/PengWCQ_LAP14_slide.pdf'>Presentation</a> ]
		</li>

		<li> 
		X. Peng, <strong>L. Wang</strong>, Z. Cai, and Y. Qiao, <br />
    Hybrid Super Vector with Improved Dense Trajectories for Action Recognition, <br />
    in <strong>THUMOS'13</strong> Action Recognition Challenge, <strong>ICCV</strong>, 2013. <br />
    [ <a href='contests/PengWCQ_Thumos13.pdf'>Paper</a> ] [ <a href='contests/PengWCQ_Thumos13.bib'>BibTex</a> ]
		</li>
    </ul>
  </div>
</div>
</div>

</body>
</html>