<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Publications</title>
	<meta content="Publications, wanglimin.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

strong, b {
	font-weight:bold;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2XT9RY7121"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2XT9RY7121');
</script>

<body>

<div style="clear: both;">
<div class="section">
  <h2>Technical Reports</h2>
  <div class="paper">
   <ul>

       <li>
   Y. Cui, C. Jiang, <strong>L. Wang</strong>, G. Wu<br />
   Target Transformed Regression for Accurate Tracking <br />
   in arXiv:2104.00403
  </li>

       <li>
           T. Li, <strong>L. Wang</strong><br />
           Learning Spatiotemporal Features via Video and Text Pair Discrimination <br />
           in arXiv:2001.05691
       </li>

  <li>
   Y. Xiong, Y. Zhao, <strong>L. Wang</strong>, D. Lin, and X. Tang<br />
   A Pursuit of Temporal Accuracy in General Activity Detection <br />
   in arXiv: 1703.03329
  </li> 


  <li>
   <strong>L. Wang</strong>, S. Guo, W. Huang, and Y. Qiao <br />
   Places205-VGGNet Models for Scene Recognition <br />
   in arXiv: 1508.01667.
  </li> 

  <li>
  <strong>L. Wang</strong>, Y. Xiong, Z. Wang, and Y. Qiao <br /> 
  Towards good practices for very deep two-stream ConvNets   <br />
  in arXiv: 1507.02159.
  </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Journal Papers</h2>
  <div class="paper">
    <ul>

        <li>
   T. Wu, M. Cao, Z. Gao, G. Wu, <strong>L. Wang</strong> <br />
   <a href=''> <strong>STMixer: A One-Stage Sparse Action Detector</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), to appear. <br />
    [ <a href=''>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/STMixer'>Code</a> ]

        <li>
   Y. Cui, C. Jiang, G. Wu, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10380715'> <strong>MixFormer: End-to-End Tracking with Iterative Mixed Attention</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), to appear. <br />
    [ <a href='https://arxiv.org/abs/2302.02814'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/MixFormer'>Code</a> ]
   </li>

                <li>
   J. Lin, Z. Liu, W. Wang, W. Wu, <strong>L. Wang</strong> <br />
   <a href=''> <strong>VLG: General Video Recognition with Web Textual Knowledge</strong> </a><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), to appear. <br />
    [ <a href=''>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
   L. Zhang, Y. Teng, <strong>L. Wang</strong> <br />
   <a href=''> <strong>Logit Normalization for Long-tail Object Detection</strong> </a><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), to appear. <br />
    [ <a href=''>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
   F. Shi, W. Huang, <strong>L. Wang</strong> <br />
   <a href='https://www.sciencedirect.com/science/article/pii/S1077314224000614'> <strong>End-to-End Dense Video Grounding via Parallel Regression</strong> </a><br />
   in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 242, 2024. <br />
    [ <a href=''>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
  Y. Ma, Y. Liu, <strong>L. Wang</strong>, W. Kang, Y. Qiao, Y. Wang <br />
   <a href='https://ieeexplore.ieee.org/document/10374139'> <strong>Dual Masked Modeling for Weakly-Supervised Temporal Boundary Discovery</strong> </a><br />
   in IEEE Transactions on Multimedia (<strong>TMM</strong>), Volume 26, Pages 5694-5704, 2024. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10374139'>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
   Y. Li, Z. Wang, Z. Li, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10458961'> <strong>Sparse Action Tube Detection</strong> </a><br />
   in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 33, Pages 1740-1752, 2024. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10458961'>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
   J. Tu, G. Wu, <strong>L. Wang</strong> <br />
   <a href='https://link.springer.com/article/10.1007/s11263-023-01901-y'> <strong>Dual Graph Networks for Pose Estimation in Crowded Scenes</strong> </a><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 132, Issue 3, Pages 633-653, 2024. <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-023-01901-y'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/DGN'>Code</a> ]
   </li>

        <li>
   H.Liu, T. Lu, Y. Xu, J. Liu, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10310261'> <strong>Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR Fusion</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), Volume 46, Issue 4, Pages 2378-2395, 2024. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10310261'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/CamLiFlow'>Code</a> ]
   </li>

        <li>
   F. Shi, R. Gao, W. Huang, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/abstract/document/10298801'> <strong>Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), Volume 46, Issue 2, Pages 1181-1198, 2024. <br />
    [ <a href='https://ieeexplore.ieee.org/abstract/document/10298801'>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

        <li>
   T. Lu, C. Liu, Y. Chen, G. Wu, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10325433'> <strong>APP-Net: Auxiliary-point-based Push and Pull Operations for Efficient Point Cloud Recognition</strong> </a><br />
   in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 32, Pages 6500-6513, 2023. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10325433'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/APP-Net'>Code</a> ]
   </li>

        <li>
   Z. Huang, G. Wu, <strong>L. Wang</strong> <br />
   <a href='https://www.sciencedirect.com/science/article/pii/S107731422300190X'> <strong>Webly-Supervised Semantic Segmentation via Curriculum Learning</strong> </a><br />
   in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 236, 2023. <br />
    [ <a href='https://www.sciencedirect.com/science/article/pii/S107731422300190X'>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>
           <li>
   Y. Tian, H. Zhang, Y. Liu, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10195242'> <strong>Recovering 3D Human Mesh from Monocular Images: A Survey</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), Volume 45, Issue 12, Pages 15406-15425, 2023. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10195242'>Paper</a> ]  [ <a href='https://github.com/tinatiansjz/hmr-survey'>Code</a> ]
   </li>

           <li>
   J. Tan, Y. Wang, G. Wu, <strong>L. Wang</strong> <br />
   <a href='https://ieeexplore.ieee.org/document/10144649'> <strong>Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection</strong> </a><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), Volume 45, Issue 10, Pages 12506-12520, 2023. <br />
    [ <a href='https://ieeexplore.ieee.org/document/10144649'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/TemporalPerceiver'>Code</a> ]
   </li>

    <li>
        M. Yang, G. Chen, Y. Zheng, T. Lu, <strong>L. Wang</strong><br />
   <a href='https://www.sciencedirect.com/science/article/pii/S1077314223000723'> <strong>BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection</strong> </a><br />
   in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 232, 2023. <br />
    [ <a href='https://www.sciencedirect.com/science/article/pii/S1077314223000723'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/BasicTAD'>Code</a> ]
   </li>

        <li>
   Z. Gao, <strong>L. Wang</strong>, G. Wu <br />
   <a href='https://link.springer.com/article/10.1007/s11263-022-01707-4'> <strong>LIP: Local Importance-based Pooling</strong> </a><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 131, Issue 1, Pages 363-384, 2023. <br />
            <alert>Journal extension of LIP with more extensive study</alert><br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-022-01707-4'>Paper</a> ]  [ <a href='https://github.com/sebgao/LIP'>Code</a> ]
   </li>

        <li>
   Y. Liu, <strong>L. Wang</strong>, Y. Wang, X. Ma, Y. Qiao<br />
   <a href='https://ieeexplore.ieee.org/document/9934010/'> <strong>FineAction: A Fine-Grained Video Dataset for Temporal Action Localization</strong> </a><br />
   in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 31, Pages 6937-6950, 2022. <br />
            <alert>A fine-grained action dataset for temporal action localization</alert><br />
    [ <a href='https://ieeexplore.ieee.org/document/9934010/'>Paper</a> ]  [ <a href='https://deeperaction.github.io/datasets/fineaction'>Dataset</a> ]
   </li>

    <li>
        Y. Cui, C. Jiang, <strong>L. Wang</strong>, G. Wu <br />
   <a href='https://www.sciencedirect.com/science/article/pii/S1077314222001254'> <strong>Fully Convolutional Online Tracking</strong> </a><br />
   in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 224, 2022. <br />
    [ <a href='https://www.sciencedirect.com/science/article/pii/S1077314222001254'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/FCOT'>Code</a> ]
   </li>

    <li>
   D. Du, J. Chen, Y. Li, K. Ma, G. Wu, Y Zheng, <strong>L. Wang</strong><br />
   <a href='https://link.springer.com/article/10.1007/s11263-022-01674-w'> <strong>Cross-Domain Gated Learning for Domain Generalization</strong> </a><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 130, Issue 11, Pages 2842–2857, 2022. <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-022-01674-w'>Paper</a> ]  [ <a href=''>Code</a> ]
   </li>

   <li>
   D. Du, <strong>L. Wang</strong>, Z. Li, G. Wu<br />
   <a href='https://link.springer.com/article/10.1007/s11263-021-01475-7'> <strong>Cross-Modal Pyramid Translation for RGB-D Scene Recognition</strong> </a><br />
   <alert>Journal extension of TRecgNet with pyramid translation extension.</alert><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 129, Issue 8, Pages 2309-2327, 2021. <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-021-01475-7'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/CMPT'>Code</a> ]
   </li> 

     <li>
   Z. Ruan, C. Zou, L. Wu, G. Wu, and <strong>L. Wang</strong><br />
   <a href='https://ieeexplore.ieee.org/document/9459461'> <strong>SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Face Alignment and Reconstruction</strong> </a><br />
   in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 30, Pages 5739-5806, 2021. <br />
    [ <a href='https://ieeexplore.ieee.org/document/9459461'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/SADRNet'>Code</a> ]
   </li> 

   <li>
   Y. Zheng, Z. Liu, Tong Lu, and <strong>L. Wang</strong><br />
   <a href='https://ieeexplore.ieee.org/document/9142434'> <strong>Dynamic Sampling Networks for Efficient Action Recognition in Videos</strong> </a><br />
   <alert>A dynamic version of TSN for efficient action recognition</alert><br />
   in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 29, Pages 7970-7983, 2020. <br />
    [ <a href='https://ieeexplore.ieee.org/document/9142434'>Paper</a> ]  [ <a href=''>BibTex</a> ]
   </li> 

   <li>
   Y. Zhao, Y. Xiong, <strong>L. Wang</strong>, Z. Wu, X. Tang, and D. Lin<br />
   <a href='https://link.springer.com/article/10.1007/s11263-019-01211-2'> <strong>Temporal Action Detection with Structured Segment Networks</strong> </a><br />
   <alert>Journal extension of SSN with more extensive study</alert><br />
   in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 128, Issue 1, Pages 74-95, 2020. <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-019-01211-2'>Paper</a> ]  [ <a href=''>BibTex</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ]
   </li> 

   <li>
   <strong>L. Wang</strong>, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool<br />
   <a href='https://ieeexplore.ieee.org/document/8454294'> <strong>Temporal Segment Networks for Action Recognition in Videos</strong> </a><br />
   <alert>More extensive study on TSN and adding performance of I3D+TSN on Kinetics</alert><br />
   in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), Volume 41, Issue 11, Pages 2740-2755, 2019. <br />
    [ <a href='https://ieeexplore.ieee.org/document/8454294'>Paper</a> ]  [ <a href=''>BibTex</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ]
   </li> 

    <li>
    B. Zhang, <strong>L. Wang</strong>, Z. Wang, Y. Qiao, and H. Wang<br />
    <a href='https://ieeexplore.ieee.org/document/8249882'><strong>Real-Time Action Recognition with Deeply-Transferred Motion Vector CNNs</strong></a><br />
    in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 27, Issue 5, Pages 2326-2339, 2018.<br />
    [ <a href='https://ieeexplore.ieee.org/document/8249882'>Paper</a> ]  [ <a href='papers/ZhangWWQW_TIP18.bib'>BibTex</a> ] [ <a href='https://github.com/zbwglory/MV-release'>Code</a> ]
    </li> 

    <li>
    <strong>L. Wang</strong>, Z. Wang, Y. Qiao, and L. Van Gool<br />
    <a href='https://link.springer.com/article/10.1007/s11263-017-1043-5'><strong>Transferring Deep Object and Scene Representations for Event Recognition in Still Images</strong></a><br />
    <alert>rank 1st place in cultural event recognition at ChaLearn LAP challenge CVPR 2015</alert><br />
    in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 126, Issue 2-4, Pages 390-409, 2018. <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-017-1043-5'>Paper</a> ]  [ <a href='papers/WangWQV_IJCV18.bib'>BibTex</a> ] [ <a href='https://github.com/wangzheallen/transfer_ijcv'>Code</a> ]
    </li> 

    <li>
    <strong>L. Wang</strong>, S. Guo, W, Huang, Y. Xiong, and Y. Qiao <br />
    <a href='https://ieeexplore.ieee.org/document/7864336'><strong>Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs</strong></a> <br />
    <alert>rank 1st place at LSUN challenge 2016 and 2nd place at Places challenge 2015</alert><br />
    in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 26, Issue 4, Pages 2055-2068, 2017. <br />
    [ <a href='https://ieeexplore.ieee.org/document/7864336'>Paper</a> ]  [ <a href='papers/WangGHXQ_TIP17.bib'>BibTex</a> ] [ <a href='https://github.com/wanglimin/MRCNN-Scene-Recognition'>Code</a> ]
    </li> 

    <li>
    Z. Wang, <strong>L. Wang</strong>, Y. Wang, B. Zhang, and Y. Qiao <br />
    <a href='https://ieeexplore.ieee.org/document/7849229'><strong>Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition</strong></a> <br />
    in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 26, Issue 4, Pages 2018-2041, 2017. <br />
    [ <a href='https://ieeexplore.ieee.org/document/7849229'>Paper</a> ]  [ <a href='papers/WangWWZQ_TIP17.bib'>BibTex</a> ] [ <a href='https://github.com/wangzheallen/vsad'>Code</a> ]
    </li> 

    <li> 
    S. Guo, W. Huang, <strong>L. Wang</strong>, and Y. Qiao<br />
    <a href='https://ieeexplore.ieee.org/document/7745887'><strong>Locally Supervised Deep Hybrid Model for Scene Recognition</strong></a> <br />
    in  IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 26, Issue 2, Pages 808-820, 2017. <br />
    [ <a href='https://ieeexplore.ieee.org/document/7745887'>Paper</a> ]  [ <a href='papers/GuoHWQ_TIP17.bib'>BibTex</a> ]
    </li> 

    <li> 
    Z. Yuan, H. Wang, <strong>L. Wang</strong>, T. Lu, P. Shivakumara, and C. L. Tan<br />
    <a href='https://www.sciencedirect.com/science/article/pii/S0957417416303591'><strong>Modeling Spatial Layout for Scene Image Understanding via a Novel Multiscale Sum-Product Network</strong></a> <br />
    in Expert Systems With Applications (<strong>ESWA</strong>), Volume 63, Pages 231-240, 2016. <br />
    [ <a href='https://www.sciencedirect.com/science/article/pii/S0957417416303591'>Paper</a> ]  [ <a href='papers/YuanWWL_ESWA16.bib'>BibTex</a> ]
    </li> 

    <li> 
    X. Peng, <strong>L. Wang</strong>, X. Wang, and Y. Qiao <br />
    <a href='https://www.sciencedirect.com/science/article/pii/S1077314216300091'><strong>Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice</strong></a> <br />
    in Computer Vision and Image Understanding (<strong>CVIU</strong>), Volume 150, Pages 109-125, 2016. <br />
    [ <a href='https://www.sciencedirect.com/science/article/pii/S1077314216300091'>Paper</a> ]  [ <a href='papers/PengWWQ_CVIU16.bib'>BibTex</a> ]
    </li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='https://link.springer.com/article/10.1007/s11263-015-0859-0'><strong>MoFAP: A Multi-Level Representation for Action Recognition</strong></a> <br />
    in International Journal of Computer Vision (<strong>IJCV</strong>), Volume 119, Issue 3, Pages 254-271, 2016.  <br />
    [ <a href='https://link.springer.com/article/10.1007/s11263-015-0859-0'>Paper</a> ]  [ <a href='papers/WangQT_IJCV16.bib'>BibTex</a> ]
		</li>

	  <li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='https://ieeexplore.ieee.org/document/6690150'><strong>Latent Hierarchical Model of Temporal Structure for Complex Activity Classification</strong></a> <br />
    in IEEE Transactions on Image Processing (<strong>TIP</strong>), Volume 23, Issue 2, Pages 810-822, 2014. <br />
    [ <a href='https://ieeexplore.ieee.org/document/6690150'>Paper</a> ]  [ <a href='papers/WangQT_TIP14.bib'>BibTex</a> ]
		</li>

    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>CVPR/ICCV/ECCV/ICLR/NeurIPS Papers</h2>
  <div class="paper">
    <ul>
        <li>
            F. Shi, J. Gu, H. Xu, S. Xu, W. Zhang, <strong>L. Wang</strong> <br />
            <a href=''><strong>BIVDiff: A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
           Z. Zhao, B. Huang, S. Xing, G. Wu, Y. Qiao, <strong>L. Wang</strong> <br />
            <a href=''><strong>Asymmetric Masked Distillation for Pre-Training Small Foundation Models</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
          Min Yang, Huan Gao, Ping Guo, <strong>L. Wang</strong> <br />
            <a href=''><strong>Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
         Y. Zhu, G. Zhang, J. Tang, G. Wu, <strong>L. Wang</strong> <br />
            <a href=''><strong>Dual DETRs for Multi-Label Temporal Action Detection</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
          C. Liu, G. Zhang, R. Zhao, <strong>L. Wang</strong> <br />
            <a href=''><strong>Sparse Global Matching for Video Frame Interpolation with Large Motion</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
          T. Wu, R. He, G. Wu, <strong>L. Wang</strong> <br />
            <a href=''><strong>SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
          Y. Huang, G. Chen, J. Xu, M. Zhang, L. Yang, B. Pei, H. Zhang, L. Dong, Y. Wang, <strong>L. Wang</strong>, Y. Qiao <br />
            <a href=''><strong>EgoBridge: A Dataset for Bridging Asynchronous First- and Third-Person View of Activities in Real World</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
          K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, <strong>L. Wang</strong>, Y. Qiao <br />
            <a href=''><strong>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
        T. Lu, M. Yu, L. Xu, Y. Xiangli, <strong>L. Wang</strong>, D. Lin, B. Dai <br />
            <a href=''><strong>Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
        Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen,  <strong>L. Wang</strong>, D. Lin, Y. Qiao, Z. Liu <br />
            <a href=''><strong>VBench: Comprehensive Benchmark Suite for Video Generative Models</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 <br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
            Z. Gao, Z. Tong, <strong>L. Wang</strong>, M. Shou <br />
            <a href='https://arxiv.org/abs/2304.03768'><strong>SparseFormer: Sparse Visual Recognition via Limited Latent Tokens</strong></a> <br />
            in The Twelfth International Conference on Learning Representations (<strong>ICLR</strong>), 2024 <br />
            [ <a href='https://arxiv.org/abs/2304.03768'>Paper</a> ] [ <a href='https://github.com/showlab/sparseformer'>Project Page</a> ]
        </li>

        <li>
            Y. Wang, Y. He, Y. Li, K. Li, J. Yu, X. Ma, X. Li, G. Chen, X. Chen, Y. Wang, C. He, P. Luo, Z. Liu, Y. Wang, <strong>L. Wang</strong>, Y. Qiao <br />
            <a href='https://arxiv.org/abs/2307.06942'><strong>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</strong></a> <br />
            in The Twelfth International Conference on Learning Representations (<strong>ICLR</strong>), 2024 <br />
            [ <a href='https://arxiv.org/abs/2307.06942'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid'>Project Page</a> ]
        </li>

        <li>
            K. Sun, J. Pan, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai, Y. Qiao, <strong>L. Wang</strong>, H. Li <br />
            <a href='https://arxiv.org/abs/2307.00716'><strong>JourneyDB: A Benchmark for Generative Image Understanding</strong></a> <br />
            in Thirty-seventh Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2307.00716'>Paper</a> ] [ <a href='https://journeydb.github.io/'>Project Page</a> ]
        </li>

        <li>
            Y. Cui, T. Song, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2305.15896'><strong>MixFormerV2: Efficient Fully Transformer Tracking</strong></a> <br />
             <alert>A CPU Real-time tracker</alert>   <br />
            in Thirty-seventh Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2305.15896'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MixFormerV2'>Code</a> ]
        </li>

        <li>
            S. Wang, Y. Teng, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2308.09564'><strong>Deep Equilibrium Object Detection</strong></a> <br />
             <alert>Extension of AdaMixer to infinite depth with DEQ</alert>  <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2308.09564'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/DEQDet'>Code</a> ]
        </li>

        <li>
            Y. Teng, H. Liu, S. Guo, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2304.04978'><strong>StageInteractor: Query-based Object Detector with Cross-stage Interaction</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2304.04978'>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
            H. Liu, Y. Teng, T. Lu, H. Wang, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2308.09244'><strong>SparseBEV: Sparse 3D Object Detection from Multi-Camera Videos</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2308.09244'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/SparseBEV'>Code</a> ]
        </li>

        <li>
            R. Gao, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2307.15700'><strong>MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2307.15700'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MeMOTR'>Code</a> ]
        </li>

        <li>
            Y. Cui, C. Zeng, X. Zhao, Y. Yang, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2304.05170'><strong>SportsMOT: A Large Multi-Object Tracking Dataset in Diverse Sports Scenes</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2304.05170'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/SportsMOT'>Code</a> ]
        </li>

        <li>
            L. Chen, Z. Tong, Y. Song, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2304.08451'><strong>Efficient Video Action Detection with Token Dropout and Context Refinement</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2304.08451'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/EVAD'>Code</a> ]
        </li>

        <li>
            B. Huang, Z. Zhao, G. Zhang, Y. Qiao, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2308.10794'><strong>MGMAE: Motion Guided Masking for Video Masked Autoencoding</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2308.10794'>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
            K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, <strong>L. Wang</strong>, Y. Qiao <br />
            <a href='https://arxiv.org/abs/2211.09552'><strong>UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2211.09552'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/UniFormerV2'>Code</a> ]
        </li>

        <li>
            K. Li, Y. Wang, Y. Li, Y. Wang, Y. He, <strong>L. Wang</strong>, Y. Qiao ( <alert>oral presentation</alert> ) <br />
            <a href='https://arxiv.org/abs/2303.16058'><strong>Unmasked Teacher: Towards Training-Efficient Video Foundation Models</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2303.16058'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/unmasked_teacher'>Code</a> ]
        </li>

        <li>
            J. Wang, G. Chen, Y. Huang, <strong>L. Wang</strong>, T. Lu <br />
            <a href='https://arxiv.org/abs/2308.07893'><strong>Memory-and-Anticipation Transformer for Online Action Understanding</strong></a> <br />
            in International Conference on Computer Vision (<strong>ICCV</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2308.07893'>Paper</a> ] [ <a href=''>Code</a> ]
        </li>

        <li>
            <strong>L. Wang</strong>, B. Huang, Z. Zhao, Z. Tong, Y. He, Y. Wang, Y. Wang, Y. Qiao <br />
            <a href='https://arxiv.org/abs/2303.16727'><strong>VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2303.16727'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/VideoMAEv2'>Code</a> ]
        </li>

        <li>
            T. Wu, M. Cao, Z. Gao, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2303.15879'><strong>STMixer: One-Stage Sparse Action Detector</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2303.15879'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/STMixer'>Code</a> ]
        </li>

        <li>
            G. Zhang, Y. Zhu, H. Wang, Y. Chen, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2303.00440'><strong>Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2303.00440'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/EMA-VFI'>Code</a> ]
        </li>


        <li>
            H. Wang, Y. Wu, S. Guo, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2303.14676'><strong>PDPP: Projected Diffusion for Procedure Planning in Instructional Videos</strong></a> ( <alert>highlight presentation</alert> )<br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023  <br />
            [ <a href='https://arxiv.org/abs/2303.14676'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/PDPP'>Code</a> ]
        </li>

        <li>
            T. Lu, X. Ding, H. Liu, G. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2303.16094'><strong>LinK: Linear Kernel for LiDAR-based 3D Perception</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 <br />
            [ <a href='https://arxiv.org/abs/2303.16094'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/LinK'>Code</a> ]
        </li>

        <li>
            H. Cheng, Z. Liu, W. Wu, <strong>L. Wang</strong> <br />
            <a href='https://openreview.net/forum?id=fiB2RjmgwQ6'><strong>Filter-Recovery Network for Multi-Speaker Audio-Visual Speech Separation</strong></a> <br />
            in International Conference on Learning Representations (<strong>ICLR</strong>), 2023 <br />
            [ <a href='https://openreview.net/forum?id=fiB2RjmgwQ6'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/BFRNet'>Code</a> ]

        </li>

        <li>
            Z. Tong, Y. Song, J. Wang, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2203.12602'><strong>VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</strong></a>  ( <alert>highlight presentation</alert> ) <br />
            in Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2203.12602'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/VideoMAE'>Code</a> ]
        </li>

        <li>
            J. Tan, X. Zhao, X. Shi, B. Kang, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2210.11035'><strong>PointTAD: Multi-Label Temporal Action Detectionwith Learnable Query Points</strong></a> <br />
            in Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022  <br />
            [ <a href='https://arxiv.org/abs/2210.11035'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/PointTAD'>Code</a> ]
        </li>

        <li>
            H. Cheng, Z. Liu, H. Zhou, C. Qian, W. Wu, <strong>L. Wang</strong> <br />
            <a href='https://arxiv.org/abs/2204.11573'><strong>Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing</strong></a> <br />
            in European Conference on Computer Vision (<strong>ECCV</strong>), 2022  <br />
            [ <a href='https://arxiv.org/abs/2204.11573'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/JoMoLD'>Code</a> ]
        </li>

        <li>
            Y. Cui, C. Jiang, <strong>L. Wang</strong>, G. Wu <br />
            <a href='https://arxiv.org/abs/2203.11082'><strong>MixFormer: End-to-End Tracking with Iterative Mixed Attention</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022  ( <alert>oral presentation</alert> ) <br />
            [ <a href='https://arxiv.org/abs/2203.11082'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MixFormer'>Code</a> ]
        </li>

        <li>
            Z. Gao, <strong>L. Wang</strong>, B. Han, S. Guo <br />
            <a href='https://arxiv.org/abs/2203.16507'><strong>AdaMixer: A Fast-Converging Query-Based Object Detector</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022  ( <alert>oral presentation</alert> ) <br />
            [ <a href='https://arxiv.org/abs/2203.16507'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/AdaMixer'>Code</a> ]
        </li>

        <li>
            L. Zhao, <strong>L. Wang</strong><br/>
            <a href='https://arxiv.org/abs/2203.15345'><strong>Task-specific Inconsistency Alignment for Domain Adaptive Object Detection</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2203.15345'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/TIA'>Code</a> ]
        </li>


        <li>
            Y. Teng, <strong>L. Wang</strong><br/>
            <a href='https://arxiv.org/abs/2106.10815'><strong>Structured Sparse R-CNN for Direct Scene Graph Generation</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2106.10815'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/Structured-Sparse-RCNN'>Code</a> ]
        </li>

        <li>
            J. Lin, H. Duan, K. Chen, D. Lin, <strong>L. Wang</strong><br/>
            <a href='https://arxiv.org/abs/2201.04388'><strong>OCSampler: Compressing Videos to One Clip with Single-step Sampling</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2201.04388'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/OCSampler'>Code</a> ]
        </li>

        <li>
            J. Tang, Z. Liu, C. Qian, W. Wu, <strong>L. Wang</strong><br/>
            <a href='https://arxiv.org/abs/2112.04771'><strong>Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2112.04771'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/DDM'>Code</a> ]
        </li>

        <li>
            S. Guo, Z. Xiong, Y. Zhong, <strong>L. Wang</strong>, X. Guo, B. Han, W. Huang <br />
            <a href='https://arxiv.org/abs/2205.13313'><strong>Cross-Architecture Self-supervised Video Representation Learning</strong></a> <br />
            in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <br />
            [ <a href='https://arxiv.org/abs/2205.13313'>Paper</a> ] [ <a href='https://github.com/guoshengcv/CACL'>Code</a> ]
        </li>

     <li>
      Y. Li, L. Chen, R. He, Z. Wang, G. Wu, <strong>L. Wang</strong><br/>
      <a href='https://arxiv.org/abs/2105.07404''><strong>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      <alert>A high-quality and fine-grained action detection benchmark</alert><br />
      [ <a href='https://arxiv.org/abs/2105.07404'>Paper</a> ] [ <a href='https://deeperaction.github.io/multisports/'>Data</a> ] [ <a href='https://github.com/MCG-NJU/MultiSports/'>Code</a> ] [ <a href='https://competitions.codalab.org/competitions/32066'>Challenge</a> ] 
    </li>

     <li>
      T. Li, <strong>L. Wang</strong>, G. Wu<br/>
      <a href='https://arxiv.org/abs/2109.04075'><strong>Self Supervision to Distillation for Long-Tailed Visual Recognition</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2109.04075'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/SSD-LT'>Code</a> ]
    </li>

    <li>
      Z. Gao, <strong>L. Wang</strong>, G. Wu<br/>
      <a href='https://arxiv.org/abs/2109.05986'><strong>Mutual Supervision for Dense Object Detection</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2109.05986'>Paper</a> ] [ <a href=''>Code (soon)</a> ]
    </li>

     <li>
      Y. Teng, <strong>L. Wang</strong>, Z. Li, G. Wu<br/>
      <a href='https://arxiv.org/abs/2108.08121'><strong>Target Adaptive Context Aggregation for Video Scene Graph Generation</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2108.08121'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/TRACE'>Code</a> ]
    </li>

     <li>
      Z. Liu, <strong>L. Wang</strong>, W. Wu, C. Qian, T. Lu <br/>
      <a href='https://arxiv.org/abs/2005.06803'><strong>TAM: Temporal Adaptive Module for Video Recognition</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2005.06803'>Paper</a> ] [ <a href='https://github.com/liu-zhy/TANet'>Code</a> ] 
    </li>

     <li>
      J. Tan, J. Tang, <strong>L. Wang</strong>, G. Wu <br/>
      <a href='https://arxiv.org/abs/2102.01894'><strong>Relaxed Transformer Decoders for Direct Action Proposal Generation</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2102.01894'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/RTD-Action'>Code</a> ] 
    </li>

     <li>
      Y. Zhi, Z. Tong, <strong>L. Wang</strong>, G. Wu <br/>
      <a href='https://arxiv.org/abs/2104.09952'><strong>MGSampler: An Explainable Sampling Strategy for Video Action Recognition</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2104.09952'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MGSampler'>Code</a> ]
    </li>

    <li>
      H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, <strong>L. Wang</strong>, Z. Sun <br/>
      <a href='https://arxiv.org/abs/2103.16507'><strong>3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop</strong></a> ( <alert>oral presentation</alert> ) <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2103.16507'>Paper</a> ] [ <a href='https://github.com/HongwenZhang/PyMAF'>Code</a> ] 
    </li>

     <li>
      T. Lu, <strong>L. Wang</strong>, G. Wu <br/>
      <a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_CGA-Net_Category_Guided_Aggregation_for_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf'><strong>CGA-Net: Category Guided Aggregation for Point Cloud Semantic Segmentation</strong></a> <br />
      in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021 <br />
      [ <a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_CGA-Net_Category_Guided_Aggregation_for_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/CGA-Net'>Code (soon)</a> ]
    </li>

     <li>
      <strong>L. Wang</strong>, Z. Tong, B. Ji, G. Wu <br/>
      <a href='https://arxiv.org/abs/2012.10071'><strong>TDN: Temporal Difference Networks for Efficient Action Recognition</strong></a> <br />
      in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021 <br />
      [ <a href='https://arxiv.org/abs/2012.10071'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/TDN'>Code</a> ] 
    </li>

    <li>
      Z. Wang, Z. Gao, <strong>L. Wang</strong>, Z. Li, G. Wu <br/>
      <a href='http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700035.pdf'><strong>Boundary-Aware Cascade Networks for Temporal Action Segmentation</strong></a> <br />
      in European Conference on Computer Vision (<strong>ECCV</strong>), 2020 <br />
      [ <a href='http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700035.pdf'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/BCN'>Code</a> ] 
    </li>

    <li>
       J. Wu, Z. Kuang, <strong>L. Wang</strong>, W. Zhang, G. Wu <br/>
      <a href='https://arxiv.org/abs/2007.09861'><strong>Context-Aware RCNN: a Baseline for Action Detection in Videos</strong></a> <br />
      in European Conference on Computer Vision (<strong>ECCV</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2007.09861'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/CRCNN-Action'>Code</a> ] 
    </li>


    <li>
       Y. Li, Z. Wang, <strong>L. Wang</strong>, G. Wu <br/>
      <a href='https://arxiv.org/abs/2001.04608'><strong>Actions as Moving Points</strong></a> <br />
      in European Conference on Computer Vision (<strong>ECCV</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2001.04608'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MOC-Detector'>Code</a> ] 
    </li>


    <li>
       C. Gao, Q. Liu, Q. Xu, <strong>L. Wang</strong>, J. Liu,  C. Zou <br />
      <a href='https://arxiv.org/abs/2003.02683'><strong>SketchyCOCO: Image Generation from Freehand Scene Sketches</strong></a> ( <alert>oral presentation</alert> ) <br />
      in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2003.02683'>Paper</a> ] [ <a href='https://github.com/sysu-imsl/EdgeGAN'>Code</a> ] 
    </li>

    <li>
      Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, <strong>L. Wang</strong> <br />
      <a href='https://arxiv.org/abs/2004.01398'><strong>TEA: Temporal Excitation and Aggregation for Action Recognition</strong></a> <br />
      in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2004.01398'>Paper</a> ] [ <a href='https://github.com/Phoenix1327/tea-action-recognition'>Code</a> ] 
    </li>

      <li>
      S. Zhang, S. Guo, W. Huang, M. Scott, <strong>L. Wang</strong> <br />
      <a href='https://arxiv.org/abs/2002.07442'><strong>V4D: 4D Convolutional Neural Networks for Video-Level Representation Learning</strong></a> <br />
      in International Conference on Learning Representations (<strong>ICLR</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2002.07442'>Paper</a> ] [ <a href='https://github.com/msight-tech/research-v4d'>Code</a> ] 
    </li>

      <li>
      Z. Gao, <strong>L. Wang</strong>, and G. Wu <br />
      <a href='https://arxiv.org/abs/1908.04156'><strong>LIP: Local Importance-based Pooling</strong></a> <br />
      in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2019. <br />
      [ <a href='https://arxiv.org/abs/1908.04156'>Paper</a> ] [ <a href=''>BibTex</a> ] [ <a href='https://github.com/sebgao/LIP'>Code</a> ] <br />
    </li> 

     <li>
    	J. Wu, <strong>L. Wang</strong>, L. Wang, J. Guo, and G. Wu <br />
    	<a href='https://arxiv.org/abs/1904.10117'><strong>Learning Actor Relation Graphs for Group Activity Recognition</strong></a> <br />
    	in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019. <br />
    	[ <a href='https://arxiv.org/abs/1904.10117'>Paper</a> ] [ <a href=''>BibTex</a> ] [ <a href='https://github.com/wjchaoGit/Group-Activity-Recognition'>Code</a> ] <br />
    </li>	

    <li>
    	D. Du, <strong>L. Wang</strong>, H. Wang, K. Zhao, G. Wu <br />
    	<a href='https://arxiv.org/abs/1904.12254''><strong>Translate-to-Recognize Networks for RGB-D Scene Recognition</strong></a> <br />
    	in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019. <br />
    	[ <a href='https://arxiv.org/abs/1904.12254'>Paper</a> ] [ <a href=''>BibTex</a> ] [ <a href='https://github.com/ownstyledu/Translate-to-Recognize-Networks'>Code</a> ] [ <a href='https://ownstyledu.github.io/Translate-to-Recognize-Networks/'>Project Page</a> ] <br />
    </li>	
    <li>
    	J. Guo, Z. Zhou, and <strong>L. Wang</strong> <br />
    	<a href='http://openaccess.thecvf.com/content_ECCV_2018/html/Jie_Guo_Single_Image_Highlight_ECCV_2018_paper.html'><strong>Single Image Highlight Removal with a Sparse and Low-Rank Reflection Model</strong></a> <br />
    	in European Conference on Computer Vision (<strong>ECCV</strong>), 2018. <br />
    	[ <a href='http://openaccess.thecvf.com/content_ECCV_2018/html/Jie_Guo_Single_Image_Highlight_ECCV_2018_paper.html'>Paper</a> ] <br />
    </li>	

    <li>
    <strong>L. Wang</strong>, W. Li, W. Li, and L. Van Gool <br />
    <a href='https://arxiv.org/abs/1711.09125'><strong>Appearance-and-Relation Networks for Video Classification</strong></a><br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br />
    [ <a href='https://arxiv.org/abs/1711.09125'>Paper</a> ] [ <a href='https://github.com/wanglimin/ARTNet'>Code</a> ] <br />
    </li>

    <li>
    Y. Zhao, Y. Xiong, <strong>L. Wang</strong>, Z. Wu, X. Tang, and D. Lin<br />
    <a href=''><strong>Temporal Action Detection with Structured Segment Networks </strong></a> <br />
    in IEEE International Conference on Computer Vision  (<strong>ICCV</strong>), 2017. <br />
    [ <a href='papers/Zhao_ICCV17.pdf'>Paper</a> ]  [ <a href='papers/Zhao_ICCV17.bib'>BibTex</a> ] [ <a href ='http://yjxiong.me/others/ssn/'>Project Page</a> ] [ <a href='https://github.com/yjxiong/action-detection'>Code</a> ]
    </li>

    <li> 
    <strong>L. Wang</strong>, Y. Xiong, D. Lin, and L. Van Gool <br />
    <a href='papers/WangXLV_CVPR17.pdf'><strong>UntrimmedNets for Weakly Supervised Action Recognition and Detection</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. <br />
    [ <a href='papers/WangXLV_CVPR17.pdf'>Paper</a> ]  [ <a href='papers/WangXLV_CVPR17.bib'>BibTex</a> ] [ <a href='papers/WangXLV_CVPR17_Poster.pdf'>Poster</a> ] [ <a href='https://github.com/wanglimin/UntrimmedNet'>Code</a> ]
    </li>

    <li> 
    J. Song, <strong>L. Wang</strong>,  L. Van Gool, and O. Hilliges <br />
    <a href='papers/SongWVH_CVPR17.pdf'><strong>Thin-Slicing Network: A Deep Structural Model for Human Pose Estimation in Videos</strong></a> ( <alert>oral presentation</alert> ) <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. <br />
    [ <a href='papers/SongWVH_CVPR17.pdf'>Paper</a> ]  [ <a href='papers/SongWVH_CVPR17.bib'>BibTex</a> ] [ <a href ='https://ait.ethz.ch/projects/2017/thin-slicing-network/'>Project Page</a> ]
    </li> 

    <li> 
    <strong>L. Wang</strong>, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool <br />
    <a href='papers/WangXWQLTV_ECCV16.pdf'><strong>Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</strong></a><br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2016. <br />
    <alert>major contribution to the winner solution of ActivityNet challenge 2016</alert><br />
    [ <a href='papers/WangXWQLTV_ECCV16.pdf'>Paper</a> ]  [ <a href='papers/WangXWQLTV_ECCV16.bib'>BibTex</a> ] [ <a href='papers/WangXWQLTV_ECCV16_Poster.pdf'>Poster</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ]
    </li>
    

    <li> 
    <strong>L. Wang</strong>, Y. Qiao, X. Tang, and L. Van Gool <br />
    <a href='papers/WangQTV_CVPR16.pdf'><strong>Actionness Estimation Using Hybrid Fully Convolutional Networks</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br />
    [ <a href='papers/WangQTV_CVPR16.pdf'>Paper</a> ]  [ <a href='papers/WangQTV_CVPR16.bib'>BibTex</a> ] [ <a href='papers/WangQTV_CVPR16_Poster.pdf'>Poster</a> ] [ <a href = 'actionness_hfcn/index.html'>Project Page</a> ] [ <a href='https://github.com/wanglimin/actionness-estimation/'>Code</a> ]
    </li>


    <li> 
    B. Zhang, <strong>L. Wang</strong>, Z. Wang, Y. Qiao, and H. Wang <br />
    <a href='papers/ZhangWWQW_CVPR16.pdf'><strong>Real-time Action Recognition with Enhanced Motion Vector CNNs</strong></a><br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. <br />
    [ <a href='papers/ZhangWWQW_CVPR16.pdf'>Paper</a> ]  [ <a href='papers/ZhangWWQW_CVPR16.bib'>BibTex</a> ] [ <a href='papers/ZhangWWQW_CVPR16_Poster.pdf'>Poster</a> ] [ <a href = 'http://zbwglory.github.io/MV-CNN/index.html'>Project Page</a> ] [ <a href="https://github.com/zbwglory/MV-release">Code</a> ]
    </li>

    <li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_CVPR15.pdf'><strong>Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2015. <br />
    [ <a href='papers/WangQT_CVPR15.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR15.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR15_Poster.pdf'>Poster<a> ] [ <a href='papers/WangQT_CVPR15_abstract.pdf'>Extended Abstract</a> ] [ <a href = 'tdd/index.html'>Project Page</a> ] [ <a href = 'https://github.com/wanglimin/TDD'>Code</a> ] 
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_ECCV14.pdf'><strong>Video Action Detection with Relational Dynamic-Poselets</strong></a> <br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2014. <br />
		[ <a href='papers/WangQT_ECCV14.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ECCV14.bib'>BibTex</a> ] [ <a href='papers/WangQT_ECCV14_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ECCV14_Spotlight.wmv'>Spotlight</a> ] [ <a href='release_plot.rar'>Code</a> ]
		</li>
    <li> 
    Z. Cai, <strong>L. Wang</strong>, X. Peng, and Y. Qiao <br />
    <a href='papers/CaiWPQ_CVPR14.pdf'><strong>Multi-View Super Vector for Action Recognition</strong></a> ( <alert>oral presentation</alert> ) <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2014. <br />
    [ <a href='papers/CaiWPQ_CVPR14.pdf'>Paper</a> ]  [ <a href='papers/CaiWPQ_CVPR14.bib'>BibTex</a> ] [ <a href='papers/CaiWPQ_CVPR14.mp4'>Video Spotlight</a> ] [ <a href='papers/CaiWPQ_CVPR14_oral.ppt'>Oral Presentation</a> ] [ <a href='papers/CaiWPQ_CVPR14_poster.ppt'>Poster</a> ] [ <a href='papers/CaiWPQ_CVPR14_Supplement.pdf'>Supplement</a> ] [ <a href='papers/CaiWPQ_CVPR14.zip'>Code</a> ]  
    </li>
    <li> 
    X. Peng*, <strong>L. Wang</strong>*, Y. Qiao, and Q. Peng (* indicates equal contribution)<br />
    <a href='papers/PengWQP_ECCV14.pdf'><strong>Boosting VLAD with Supervised Dictionary Learning and High-Order Statistics</strong></a> <br />
    in European Conference on Computer Vision (<strong>ECCV</strong>), 2014. <br />
    [ <a href='papers/PengWQP_ECCV14.pdf'>Paper</a> ] [ <a href='papers/PengWQP_ECCV14.bib'>BibTex</a> ] 
    </li>
		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_ICCV13.pdf'><strong>Mining Motion Atoms and Phrases for Complex Action Recognition</strong></a> <br />
     in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2013. <br />
		[ <a href='papers/WangQT_ICCV13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ICCV13.bib'>BibTex</a> ] [ <a href='papers/WangQT_ICCV13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ICCV13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'mofap/index.html'>Project Page</a> ] 
		</li>
		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    <a href='papers/WangQT_CVPR13.pdf'><strong>Motionlets: Mid-Level 3D Parts for Human Motion Recognition</strong></a> <br />
    in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2013. <br />
		[ <a href='papers/WangQT_CVPR13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR13.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_CVPR13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'motionlet/index.html'>Project Page</a> ]
		</li>
		
	</ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Other Conference Papers</h2>
  <div class="paper">
    <ul>
        <li>
      H. Zhang, Y. Liu, Y. Wang, <strong>L. Wang</strong>, Y Qiao <br />
      <a href=''><strong>Learning Discriminative Feature Representation for Open Set Action Recognition</strong></a> <br />
      in ACM Multimedia Conference (<strong>ACM MM</strong>), 2023 <br />
      [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
    </li>

        <li>
      Y. Feng, Z. Zhang, R. Quan, <strong>L. Wang</strong>, J Qin <br />
            <a href=''><strong>RefineTAD: Learning Proposal-free Refinement for Temporal Action Detection</strong></a> (<alert>Honorable Mention Award</alert>)<br />
      in ACM Multimedia Conference (<strong>ACM MM</strong>), 2023 <br />
      [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ]
    </li>

    <li>
      J. Yang, S. Guo, G. Wu, <strong>L. Wang</strong> <br />
      <a href='https://arxiv.org/abs/2302.06148'><strong>CoMAE: Single Model Hybrid Pre-training on Small-Scale RGB-D Datasets</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2023 <br />
      [ <a href='https://arxiv.org/abs/2302.06148'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/CoMAE'>Code</a> ]
    </li>

    <li>
      G. Chen, Y. Zhen, <strong>L. Wang</strong>, and T. Lu <br />
      <a href='https://arxiv.org/abs/2112.03612'><strong>DCAN: Improving Temporal Action Detection via Dual Context Aggregation</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022 <br />
      [ <a href='https://arxiv.org/abs/2112.03612'>Paper</a> ] [ <a href='https://github.com/cg1177/DCAN'>Code</a> ]
    </li>
	    
    <li>
      Z. Wang, <strong>L. Wang</strong>, T. Wu, T. Li, and G. Wu <br />
      <a href='https://arxiv.org/abs/2109.04872'><strong>Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022 <br />
      [ <a href='https://arxiv.org/abs/2109.04872'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/DMN'>Code</a> ] 
    </li>
	   
    <li>
    Z. Zhu, <strong>L. Wang</strong>, S. Guo, and G. Wu<br />
    <a href='https://arxiv.org/abs/2110.12358'><strong>A Closer Look at Few-Shot Video Classification: A New Baseline and Benchmark</strong></a> <br />
    in British Machine Vision Conference (<strong>BMVC</strong>), 2021. <br />
    [ <a href='https://arxiv.org/abs/2110.12358'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/FSL-Video'>Code</a> ] 
    </li> 
	    
    <li>
      Z. Liu, D. Luo, Y. Wang, <strong>L. Wang</strong>, Y. Tai, C. Wang, J. Li, F. Huang, T. Lu <br />
      <a href='https://arxiv.org/abs/1911.09435'><strong>TEINet: Towards an Efficient Architecture for Video Recognition</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/1911.09435'>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li>

    <li>
      S. Zhang, S. Guo, <strong>L. Wang</strong>, W. Huang, M. Scott <br />
      <a href='https://arxiv.org/abs/2002.07471'><strong>Knowledge Integration Networks for Action Recognition</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020 <br />
      [ <a href='https://arxiv.org/abs/2002.07471'>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li>

    <li>
      Y. Li, W. Lin, T. Wang, J. See, R. Qian, N. Xu, <strong>L. Wang</strong>, S. Xu <br />
      <a href=''><strong>Finding Action Tubes with a Sparse-to-Dense Framework</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020 <br />
      [ <a href=''>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li>

    <li>
      Y. Yao, Z. Sun, F. Shen, L. Liu, <strong>L. Wang</strong>, F. Zhu, L. Ding, G. Wu, L. Shao <br />
      <a href='https://arxiv.org/abs/1905.10955'><strong>Dynamically Visual Disambiguation of Keyword-based Image Search</strong></a> <br />
      in International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2019 <br />
      [ <a href='https://arxiv.org/abs/1905.10955'>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li>

    <li>
      D. He, Z. Zhou, C. Gan, F. Li, X. Liu, Y. Li, <strong>L. Wang</strong>, S. Wen <br />
      <a href='https://arxiv.org/abs/1811.01549'><strong>StNet: Local and Global Spatial-Temporal Modeling for Action Recognition</strong></a> <br />
      in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2019 <br />
      [ <a href='https://arxiv.org/abs/1811.01549'>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li>

   <li>
    Z. Wang, X. Liu, L. Chen, <strong>L. Wang</strong>, Y. Qiao, X. Xie, and C. Fowlkes<br />
    <a href='papers/'><strong>Structed Triplets Learning with Pos-tag Guided Attention for Visual Question Answering</strong></a> <br />
    in IEEE Winter Conference on Applications of Computer Vision  (<strong>WACV</strong>), 2018. <br />
    [ <a href=''>Paper</a> ] [ <a href=''>BibTex</a> ] 
    </li> 

    <li>
    Y. Wang, J. Song, <strong>L. Wang</strong>, L. Van Gool, and O. Hilliges<br />
    <a href='papers/WangSWVH_BMVC16.pdf'><strong>Two-Stream SR-CNNs for Action Recognition in Videos</strong></a> <br />
    in British Machine Vision Conference (<strong>BMVC</strong>), 2016. <br />
    [ <a href='papers/WangSWVH_BMVC16.pdf'>Paper</a> ] [ <a href='papers/WangSWVH_BMVC16.bib'>BibTex</a> ] 
    </li> 

    <li>
    Z. Wang, Y. Wang, <strong>L. Wang</strong>, and Y. Qiao <br />
    <a href='papers/WangWWQ_ICASSP16.pdf'><strong>Codebook Enhancement of VLAD Representation for Visual Recognition</strong></a> <br />
    in IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2016. <br />
    [ <a href='papers/WangWWQ_ICASSP16.pdf'>Paper</a> ] [ <a href='papers/WangWWQ_ICASSP16.bib'>BibTex</a> ] 
    </li> 

		<li> 
		X. Peng, <strong>L. Wang</strong>, Y. Qiao, and Q. Peng <br />
    <a href='papers/PengWQP_ICPR14.pdf'><strong>A Joint Evaluation of Dictionary Learning and Feature Encoding for Action Recognition</strong></a> <br />
    in International Conference on Pattern Recognition (<strong>ICPR</strong>), 2014. <br />
    [ <a href='papers/PengWQP_ICPR14.pdf'>Paper</a> ] [ <a href='papers/PengWQP_ICPR14.bib'>BibTex</a> ] 
		</li>

		<li> 
		X. Wang, <strong>L. Wang</strong>, and Y. Qiao <br />
    <a href='papers/WangWQ_ACCV12.pdf'><strong>A Comparative Study of Encoding, Pooling and Normalization Methods for Action Recognition</strong></a> <br />
    in Asian Conference on Computer Vision (<strong>ACCV</strong>), 2012. <br />
    [ <a href='papers/WangWQ_ACCV12.pdf'>Paper</a> ] [ <a href='papers/WangWQ_ACCV12.bib'>BibTex</a> ] [ <a href='papers/WangWQ_ACCV12_Poster.pdf'>Poster</a> ] [ <a href='papers/WangWQ_ACCV12_Spotlight.ppt'>Spotlight</a> ]
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Wu, T. Lu, and K. Chen <br />
    <a href='papers/WangWLC_MM11.pdf'><strong>Multiclass Object Detection by Combining Local Appearances and Context</strong></a> <br />
    in ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2011. <br />
    [ <a href='papers/WangWLC_MM11.pdf'>Paper</a> ] [ <a href='papers/WangWLC_MM11.bib'>BibTex</a> ]
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Wu, Z. Tian, Z. Sun, and T. Lu <br />
    <a href='papers/WangWTSL_PCM10.pdf'><strong>A Novel Approach for Robust Surveillance Video Content Abstraction</strong></a> <br />
    in Pacific-Rim Conference on Multimedia (<strong>PCM</strong>), 2010. <br />
    [ <a href='papers/WangWTSL_PCM10.pdf'>Paper</a> ] [ <a href='papers/WangWTSL_PCM10.bib'>BibTex</a> ]
		</li>

    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Workshop and Notebook Papers</h2>
  <div class="paper">
    <ul>

    <li>
      Y. Xiong, <strong>L. Wang</strong>, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao, L. Van Gool, and X. Tang <br />
      CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 ( <alert>rank 1st place</alert> ) <br />
      in ActivityNet Large Scale Activity Recognition Challenge, <strong>CVPR</strong>, 2016. <br />
      [ <a href='contests/XiongW_ActivityNet16.pdf'>Paper</a> ] [ BibTex] [ Presentation ] <br /> 
    </li>

    <li>
    <strong>L. Wang</strong>, Z. Wang, S. Guo, and Y. Qiao <br />
    Better Exploiting OS-CNNs for Better Event Recognition in Images <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>ICCV</strong>, 2015. <br />
    [ <a href='contests/WangWGQ_ChaLearnLAP15.pdf'>Paper</a>  ] [ <a href='contests/WangWGQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='contests/WangWGQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ]
    </li>

		<li>
		<strong>L. Wang</strong>, Z. Wang, Y. Xiong, and Y. Qiao <br />
    CUHK&SIAT Submission for THUMOS15 Action Recognition Challenge <br /> 
    in <strong>THUMOS'15</strong> Action Recognition Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWXQ_THUMOS15.pdf'>Paper</a> ] [ <a href='contests/WangWXQ_Thumos15.bib'>BibTex</a> ] [ <a href='contests/WangWXQ_Thumos15_slide.pdf'>Presentation</a> ] <br /> 
		</li>

		<li>
		<strong>L. Wang</strong>, Z. Wang, W. Du, and Y. Qiao <br />
    Object-Scene Convolutional Neural Networks for Event Recognition in Images ( <alert>rank 1st place</alert> )<br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWDQ_ChaLearnLAP15.pdf'>Paper</a>  ]  [ <a href='contests/WangWDQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='contests/WangWDQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ] <br /> 
		</li>

		<li>
		Z. Wang, <strong>L. Wang</strong>, W. Du, and Y. Qiao <br />
    Exploring Fisher Vector and Deep Networks for Action Spotting ( <alert>rank 1st place</alert> ) <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>CVPR</strong>, 2015. <br />
    [ <a href='contests/WangWDQ_LAP15.pdf'>Paper</a>  ]  [ <a href='contests/WangWDQ_LAP15.bib'>BibTex</a> ] [ <a href='contests/WangWDQ_LAP15_slide.pdf'>Presentation</a> ] <br /> 
		</li>

		<li> 
		<strong>L. Wang</strong>, Y. Qiao, and X. Tang <br />
    Action Recognition and Detection by Combining Motion and Appearance Features, <br />
    in <strong>THUMOS'14</strong> Action Recognition Challenge, <strong>ECCV</strong>, 2014. <br />
    [ <a href='contests/WangQT_Thumos14.pdf'>Paper</a> ] [ <a href='contests/WangQT_Thumos14.bib'>BibTex</a> ]  [ <a href='contests/WangQT_Thumos14_slide.pdf'>Presentation</a> ] 
		</li>

		<li> 
		X. Peng, <strong>L. Wang</strong>, Z. Cai, and Y. Qiao <br />
    Action and Gesture Temporal Spotting with Super Vector Representation ( <alert>rank 1st place</alert> ) <br />
    in ChaLearn Looking at People (<strong>LAP</strong>) Challenge, <strong>ECCV</strong>, 2014. <br />
    [ <a href='contests/PengWCQ_LAP14.pdf'>Paper</a> ] [ <a href='contests/PengWCQ_LAP14.bib'>BibTex</a> ]  [ <a href='contests/PengWCQ_LAP14_slide.pdf'>Presentation</a> ]
		</li>

		<li> 
		X. Peng, <strong>L. Wang</strong>, Z. Cai, and Y. Qiao, <br />
    Hybrid Super Vector with Improved Dense Trajectories for Action Recognition, <br />
    in <strong>THUMOS'13</strong> Action Recognition Challenge, <strong>ICCV</strong>, 2013. <br />
    [ <a href='contests/PengWCQ_Thumos13.pdf'>Paper</a> ] [ <a href='contests/PengWCQ_Thumos13.bib'>BibTex</a> ]
		</li>
    </ul>
  </div>
</div>
</div>

</body>
</html>
