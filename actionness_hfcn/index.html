<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Actionness Estimation</title>
	<meta content="Limin, wanglimin.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h1 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  # font-size: 16px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

img.pipeline {
  float: center;
  width: 770px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wanglimin.github.io');
  ga('send', 'pageview');

</script>
<body>

<div align = "center">
<h1>Actionness Estimation Using Hybrid Fully Convolutional Networks</h1> <br />
<h3>Limin Wang, Yu Qiao, Xiaoou Tang, and Luc Van Gool</h3>
</div>

<div style="clear: both;">
<div class="paper">
  <img class="pipeline" src="actionness.png" title="actionness" />
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Abstract</h2>
  <div class="paper">
  Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Method</h2>
  <div class="paper">
  As shown in the above figure, we propose the architecture of hybrid fully convolutional networks (H-FCN) for actionness estimation in videos. In addition, based on actionness maps, we develop a action proposal generation method and present a more unified action detection framework. 
		  <br><br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Hybrid fully convolutional networks: </strong> H-FCN is composed of appearance fully convolutional network (A-FCN) and motion fully convolutional network (M-FCN). These two FCNs capture visual information for actionness estimation from the perspectives of static appearance and dynamic motion, respectively.
		  </li>
		  <br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Actionness estimation: </strong> A-FCN takes a single RGB image as input and M-FCN deals with two consecutive optical flow fields. In order to deal with scale variations,  We construct pyramid representations of RGB frames and stacking optical flow fields. These actionness maps from different scales are first up-sampled to the size of original image and then averaged.
		  </li>
		  <br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Action proposal generation: </strong> Actionness maps are generic visual features and can be exploited for different vision problems. Here, we propose a sampling method to generate action proposals based on estimated actionness maps. We sample boxes according to their scores and spatial overlaps.
		  </li>
      <br>
      <li style = "list-style-type:square; margin-left:0px">
      <strong>Action detection: </strong> Following R-CNN, we train an action classifier with two-stream CNNs by cropping positive examples and mining negative examples. At test time, we directly use the output of two-stream CNNs as the detection score for each action proposal.
      </li>
		  <br>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Results</h2>
  <div class="paper">
  <ul>
  <li><h3>Examples of Actionness Maps and Action Proposals</h3></li>
  <img src="example.png"  width="770"/>

  <li><h3>Evaluation on Actionness Estimation (UCF Sports, Stanford40, and J-HMDB)</h3> </li>
  <div align=center>
  <img src="actionness1.png"  width="360" /> 
  <img src="actionness2.png"  width="350" />
  </div>

  <li><h3>Evaluation on Action Proposal (Stanford 40 and J-HMDB)</h3></li>
  <img src="proposal.png"  width="770"/>

  <li><h3>Evaluation on Action Detection (J-HMDB)</h3></li>
  <img src="detection.png"  width="770"/>
  
  </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Downloads</h2>
  <div class="paper">
  <ul>
  <li><h3>The code of TDD extraction:</h3> </li>
   Code on github [ <a href='https://github.com/wanglimin/actionness-estimation/'>Link</a> ]
   </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>References</h2>
  <div class="paper">
  If you use our trained model or code, please cite the following paper: <br> <br>
  Limin Wang, Yu Qiao, Xiaoou Tang, and Luc Van Gool <br>
  Actionness estimation using hybrid fully convolutional networks <br>
  in IEEE conference on computer vision and pattern recognition (<strong>CVPR</strong>), 2016
  </div>
</div>
</div>


</div>
</div>
<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 20th July, 2016</a></font></p>
</div>
</body>
</html>
