<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Limin Wang</title>
	<meta content="Limin Wang, wanglimin.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2XT9RY7121"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2XT9RY7121');
</script>

<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
<div style="margin: 0px auto; width: 100%;">
<img title="lmwang" style="float: left; padding-left: .01em; height: 130px;" src="wlm.JPG" />
<div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 15pt;">Limin Wang (王利民)</span><br />
<span><a href='http://mcg.nju.edu.cn/en/index.html'>Multimedia Computing Group</a></span> <br />
<span><a href='http://cs.nju.edu.cn'>Department of Computer Science and Technology</a></span> <br />
<span><a href='http://www.nju.edu.cn'>Nanjing University</a></span><br />
<span><strong>Office</strong>: CS Building 506 </span><br />
<span><strong>Email</strong>: lmwang.nju [at] gmail.com</span> <br /> 
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me (<a href='wlm_cv.pdf'>CV</a>)</h2>
<div class="paper">
I am a Professor at <a href='http://cs.nju.edu.cn'>Department of Computer Science and Technology</a> and also affiliated with <a href='https://keysoftlab.nju.edu.cn/'>State Key Laboratory for Novel Software Technology</a>, Nanjing University.
<br>
<br>
Previously, I received the B.S. degree from <a href='http://www.nju.edu.cn/'>Nanjing University</a> in 2011, and the Ph.D. degree from <a href='http://www.cuhk.edu.hk/chinese/index.html'>The Chinese University of Hong Kong</a> under the supervision of Prof. <a href='http://www.ie.cuhk.edu.hk/people/xotang.shtml'>Xiaoou Tang</a> in 2015. From 2015 to 2018, I was a Post-Doctoral Researcher with Prof. <a href="http://www.vision.ee.ethz.ch/members/get_member.cgi?lang=en&id=1">Luc Van Gool</a> in the <a href='http://www.vision.ee.ethz.ch/en/'>Computer Vision Laboratory</a> (CVL) at <a href="https://www.ethz.ch/en.html">ETH Zurich</a>. 
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
      <li> 2025-06-27: Five papers are accepted by ICCV 2025.</li>
      <li> 2025-05-01: Two papers are accepted by ICML 2025.</li>
      <li> 2025-04-19: <alert> Prof. Limin Wang is invited to be an Associate Editor of T-PAMI. </alert></li>
      <li> 2025-03-28: <a href="https://ieeexplore.ieee.org/document/10949703">JointFormer</a> is accepted by T-PAMI.</li>
      <li> 2025-02-27: Six papers are accepted by CVPR 2025.</li>
      <li> 2025-01-23: Six papers are accepted by ICLR 2025.</li>
      <li> 2024-12-12: The <a href="https://arxiv.org/abs/2303.14676">extension </a> of PDPP is accepted by T-PAMI.</li>
      <li> 2024-09-26: Four papers are accepted by NeurIPS 2024. </li>
      <li> 2024-07-01: <a href="https://arxiv.org/abs/2403.15377">InternVideo2</a> is accepted by ECCV 2024. </li>
      <li> 2024-07-01: Six papers are accepted by ECCV 2024 on video foundation model & image generation etc.</li>
      <li> 2024-06-01: <alert> Our MixFormer is selected as the Featured Article of TPAMI. </alert> </li>
      <li> 2024-04-12: The <a href="https://arxiv.org/abs/2212.01638">VLG work </a> is accepted by IJCV.</li>
      <li> 2024-04-07: The <a href="https://arxiv.org/abs/2303.15879">extension </a> of STMixer is accepted by T-PAMI.</li>
      <li> 2024-02-27: Ten papers are accepted by CVPR 2024 on video foundation model  & video generation & benchmarks etc.</li>
      <li> 2024-01-15: Two papers (<a href="https://arxiv.org/abs/2307.06942">InternVid</a>  & <a href="https://arxiv.org/abs/2304.03768">SparseFormer</a>) are accepted by ICLR 2024.</li>
      <li> 2024-01-01: The <a href="https://arxiv.org/abs/2302.02814">extension </a> of MixFormer is accepted by T-PAMI.</li>
      <li> 2023-11-30: Our <a href="https://arxiv.org/abs/2203.17020">LogN</a> is accepted by IJCV. </li>
      <li> 2023-11-01: Our <a href="https://arxiv.org/abs/2303.12017">CamLiFlow</a> is accepted by T-PAMI. </li>
      <li> 2023-11-01: <alert> Our RefineTAD receives the <a href='https://www.acmmm2023.org/'>Best Paper Honorable Mention Award</a> of ACM MM 2023. </alert> </li>
      <li> 2023-10-25: Our <a href='https://arxiv.org/abs/2209.13959'>Dynamic MDETR</a> is accepted by T-PAMI.
      <li> 2023-09-22: Our <a href='https://arxiv.org/abs/2305.15896'>MixFormer V2</a> is accepted by NeurIPS 2023.
      <li> 2023-09-02: One paper on crowded pose estimation is accepted by IJCV.  </li>
      <li> 2023-07-21: Our  <a href='https://arxiv.org/abs/2203.01923'>survey paper</a> on 3D human mesh recovery is accepted by T-PAMI.  </li>
      <li> 2023-07-14: Our <a href='https://github.com/OpenGVLab/unmasked_teacher'>UMT Foundation Model</a> is accepted by ICCV 2023.  </li>
      <li> 2023-07-14: Our <a href='https://github.com/MCG-NJU/SportsMOT'>SportsMOT dataset</a> is accepted by ICCV 2023.  </li>
      <li> 2023-07-14: <a href='publication.html'>Ten papers</a> are accepted by ICCV 2023 (Topics: Video foundation models, action detection and anticipation, multi-object tracking, (3D) object detection, new dataset.) </li>
      <li> 2023-07-13: We release the  <a href='https://arxiv.org/abs/2307.06942'> InterVid </a> dataset for multi-modal video understanding and generation.  </li>
      <li> 2023-06-25: We release the  <a href='https://arxiv.org/abs/2306.05716'> Grasp Anything </a> project for embodied AI by leveraging vision foundation model. </li>
      <li> 2023-06-15: <alert>Prof. Limin Wang is invited to be an <a href='https://www.springer.com/journal/11263/editors'>Editorial Board Member</a> of IJCV. </alert> </li>
      <li> 2023-06-10: I am invited to give a ARP talk at <a href="http://valser.org/2023/#/" >VALSE 2023</a> (<a href='talks/arp2023.pdf'>slide</a>).</li>
      <li> 2022-05-25: We propose the  <a href='https://arxiv.org/abs/2305.15896'>MixFormer V2</a>, a real-time object tracker. We have released the <a href="https://github.com/MCG-NJU/MixFormerV2">source code</a>. </li>
      <li> 2023-05-19: <a href='https://ieeexplore.ieee.org/document/10144649'>Temporal Perceiver</a> is accepted by T-PAMI. We have released the <a href='https://github.com/MCG-NJU/TemporalPerceiver'>source code</a>.</li>
      <li> 2023-05-10: We present the <a href='https://github.com/OpenGVLab/Ask-Anything'>VideoChat</a> system, by combining video foundation model and LLM. </li>
      <li> 2023-03-18: We propose the <a href='https://arxiv.org/abs/2303.16727'>VideoMAE V2</a>, training the first billion-level video transformer (<a href='https://github.com/OpenGVLab/VideoMAEv2'>source code</a>). </li>
      <li> 2023-03-01: Five papers on video understanding and point cloud analysis are accepted by CVPR 2023.</li>
      <li> 2023-02-01: One paper is accepted by ICLR 2023 and one by AAAI 2023.</li>
      <li> 2022-11-01: The <a href='https://deeperaction.github.io/datasets/fineaction'>FineAction</a> dataset is accepted by TIP.</li>
      <li> 2022-10-09: The extension of <a href='https://link.springer.com/article/10.1007/s11263-022-01707-4'>LIP</a> is accepted by IJCV.</li>
      <li> 2022-09-15: <a href='https://arxiv.org/abs/2203.12602'>VideoMAE</a> and <a href='https://arxiv.org/abs/2210.11035'>PointTAD</a> are accepted by NeurIPS 2022.</li>
      <li> 2022-09-15: We present the <a href='https://arxiv.org/abs/2205.02717'>BasicTAD</a>, an end-to-end TAD baseline method. We have released the <a href='https://github.com/MCG-NJU/BasicTAD'>source code</a>. </li>
      <li> 2022-08-10: One paper is accepted by ECCV 2022 and one paper (<a href="https://link.springer.com/article/10.1007/s11263-022-01674-w">CDG</a>) is accepted by IJCV.</li>
      <li> 2022-05-01: We are organizing the second <a href='https://deeperaction.github.io'>DeeperAction Challenge</a> at ECCV 2022, by introducing five new benchmarks on temporal action localization, multi-actor tracking, spatiotemporal action detection, part-level action parsing, and fine-grained video anomaly recognition. </li>
      <li> 2022-03-23: We present the <a href='https://arxiv.org/abs/2203.12602'>VideoMAE</a>, a self-supervised video transformer obtaining SOTA performance on the benchmarks of Kinetics, Something-Something, and AVA. We have released the <a href='https://github.com/MCG-NJU/VideoMAE'>source code</a> and <a href="https://github.com/MCG-NJU/VideoMAE/blob/main/MODEL_ZOO.md"> pre-trained models</a>.<alert> </li>
      <li> 2022-03-02: We present the  <a href='https://arxiv.org/abs/2203.11082'>MixFormer</a>, a compact and efficient object tracker, obtaining SOTA performance on several benchmarks. We have released the <a href="https://github.com/MCG-NJU/MixFormer">source code</a>. </li>
      <li> 2022-03-02: We present the  <a href='https://arxiv.org/abs/2203.16507'>AdaMixer</a>, a fast-converging query based object detector tracker, obtaining competitive performance on the MS COCCO benchmark. We have released the <a href="https://github.com/MCG-NJU/AdaMixer">source code</a>. </li>
      <li> 2022-03-02: Seven papers on object detection, object tracking, action recognition etc. are accepted by CVPR 2022.</li>
      <li> 2021-07-25: Eight papers on video understanding are accepted by ICCV 2021: new dataset (<a href='https://arxiv.org/abs/2105.07404'>MultiSports</a>), backbone (<a href='https://arxiv.org/abs/2005.06803'>TAM</a>), sampling method (<a href='https://arxiv.org/abs/2104.09952'>MGSampler</a>), detection frameworks (<a href='https://arxiv.org/abs/2102.01894'>RTD</a> and <a href='https://arxiv.org/abs/2108.08121'>TRACE</a>). For more details, please refer to our papers.
    <li> 2021-07-15:   We release the <a href='https://deeperaction.github.io/multisports/'>MultiSports dataset</a> for spatiotemporal action detection.  </li>
    <li> 2021-07-15: Our team secures the first place at <a href='http://auto-video-captions.top/2021/'>ACM MM Pre-training for Video Understanding Challenge</a> for Track 2. </li> 
    <li> 2021-06-15: Our team secures the first place at <a href='https://eval.ai/web/challenges/challenge-page/1054/overview'>CVPR Kinetics Challenge</a> for Self-Supervised Task. </li>
    <li> 2021-06-15: Our team secures the first place at <a href='http://www.picdataset.com/challenge/task/hcvg/'>CVPR PIC Challenge</a> for Human-Centric Spatio-Temporal Video Grounding Task. </li>
    <li> 2021-06-01: <alert> We are organizing <a href='https://deeperaction.github.io'>DeeperAction Challenge</a> at ICCV 2021, by introducing three new benchmarks on temporal action localization, spatiotemporal action detection, and part-level action parsing.  </alert> </li>
    <li> 2021-04-20: The extension of TRecgNet is accepted by IJCV. </li>
    <li> 2021-04-07: We propose a target transformer for accurate anchor-free tracking, termed as <a href='https://arxiv.org/abs/2104.00403'>TREG</a> (<a href="https://github.com/MCG-NJU/TREG">code</a>). </li>
    <li> 2021-04-07: We present a transformer decoder for direct action proposal generation, termed as <a href='https://arxiv.org/abs/2102.01894'>RTD-Net</a> (<a href="https://github.com/MCG-NJU/RTD-Action">code</a>). </li>
    <li> 2021-03-01: Two papers on action recognition and point cloud segmentation are accepted by CVPR 2021. </li>	
    <li> 2020-12-30: We propose a new video architecture of using temporal difference, termed as <a href='https://arxiv.org/abs/2012.10071'>TDN</a> and realease the <a href='https://github.com/MCG-NJU/TDN'>code</a>. </li>
    <li> 2020-07-03: Three papers on action detection and segmentation are accepted by ECCV 2020. </li>	
    <li> 2020-06-28: Our proposed <a href='https://arxiv.org/abs/2006.15560'>DSN</a>, a dynamic version of TSN for efficient action recognition, is accepted by TIP. </li>	
    <li> 2020-05-14: We propose a temporal adaptive module for video recognition, termed as <a href='https://arxiv.org/abs/2005.06803'>TAM</a> and <a href='https://github.com/liu-zhy/TANet'>code</a>. </li>
    <li> 2020-04-16: The code of our published papers will be made available at <a href='https://github.com/MCG-NJU'>Github: MCG-NJU</a>. </li>  
    <li> 2020-04-16: We propose a fully convolutional online tracking framwork, termed as <a href='https://arxiv.org/abs/2004.07109'>FCOT</a> and <a href='https://github.com/MCG-NJU/FCOT'>code</a>. </li>
    <li> 2020-03-10: Our proposed temporal module <a href='https://arxiv.org/abs/2004.01398'>TEA</a> is accepted by CVPR 2020. </li>
    <li> 2020-01-20: We propose an efficient video representation learning framwork, termed as <a href='https://arxiv.org/abs/2001.05691'>CPD</a> and release the <a href='https://github.com/MCG-NJU/CPD-Video'>code</a>. </li>
    <li> 2020-01-15: We present an anchor-free action tubelet detector, termed as <a href='https://arxiv.org/abs/2001.04608'>MOC-Detector</a> and release the <a href='https://github.com/MCG-NJU/MOC-Detector'>code</a>. </li>
    <li> 2019-12-20: Our proposed <a href='https://arxiv.org/abs/2002.07442'> V4D</a>, a principled video-level representation learning framework, is accepted by ICLR 2020. </li>
    <li> 2019-11-21: Our proposed <a href='https://arxiv.org/abs/1911.09435'>TEINet</a>, an efficient video architecture for video recognition, is accepted by AAAI 2020. </li>
    <li> 2019-07-23: Our proposed <a href='https://arxiv.org/abs/1908.04156'>LIP</a>, a general alternative to average or max pooling, is accepted by ICCV 2019. </li>
    <li> 2019-03-15: Two papers are accepted by CVPR 2019: one for group activity recognition and one for RGB-D transfer learning. </li>
    <li> 2018-08-19: One paper is accepted by ECCV 2018 and one (<a href="https://ieeexplore.ieee.org/document/8454294">TSN</a>) by T-PAMI. </li>
    <li> 2018-04-01: <alert>I join <a href='https://www.nju.edu.cn/'>Nanjing University</a> as a faculty member at <a href='http://cs.nju.edu.cn/'>Department of Computer Science and Technology</a></alert>. </li>
    <li> 2017-11-28: We released a recent work on video architecture design for spatiotemporal feature learning. [ <a href='https://arxiv.org/abs/1711.09125'>arXiv</a> ] [ <a href='https://github.com/wanglimin/ARTNet'>Code</a> ].  </li>
    <li> 2017-09-08: We have released the TSN models learned in the <a href='http://yjxiong.me/others/kinetics_action/'>Kinetics</a> dataset. These models could be transferred well to the existing datasets for action recognition and detection [ <a href='http://yjxiong.me/others/kinetics_action/'>Link</a> ].  </li>
    <li> 2017-09-01: One paper is accepted by ICCV 2017 and one (<a href="https://link.springer.com/article/10.1007/s11263-017-1043-5">OS2E-CNN</a>) by IJCV. </li>
    <li> 2017-07-18: I am invited to give a talk at the <a href='https://sites.google.com/view/fvt2017/home'>Workshop on Frontiers of Video Technology-2017</a> [ <a href='fvt_slide.pdf'>Slide</a> ].
    <li> 2017-03-28: <alert>I am co-organizing the CVPR2017 workshop and challenge on Visual Understanding by Learning from Web Data</alert>. For more details, please see the <a href='http://www.vision.ee.ethz.ch/webvision/workshop.html'>workshop page</a> and <a href='https://competitions.codalab.org/competitions/16439'>challenge page</a>. </li>
    <li> 2017-02-28: Two papers are accepted by CVPR 2017. </li>
    <li> 2016-12-20: <alert>We release the code and models for SR-CNN paper</alert> [ <a href='https://github.com/yifita/action.sr_cnn'>Code</a> ]. </li> 
    <li> 2016-10-05: <alert>We release the code and models for Places2 scene recognition challenge</alert> [ <a href='https://arxiv.org/abs/1610.01119'>arXiv</a> ] [ <a href='https://github.com/wanglimin/MRCNN-Scene-Recognition'>Code</a> ]. </li>
    <li> 2016-08-03: <alert>Code and model of Temporal Segment Networks is released</alert> [ <a href='http://arxiv.org/abs/1608.00859'>arXiv</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ]. </li>
    <li> 2016-07-15: One paper is accepted by ECCV 2016 and one by BMVC 2016. </li>
    <li> 2016-06-16: <alert>Our team secures the 1st place for untrimmed video classification at ActivityNet Challenge 2016</alert> [ <a href='http://activity-net.org/challenges/2016/program.html'>Result</a> ]. <br>
    Basically, our solution is based on our works of <a href='https://github.com/yjxiong/temporal-segment-networks'>Temporal Segment Networks</a> (TSN) and <a href = 'https://github.com/wanglimin/TDD'>Trajectory-pooled Deep-convolutional Descriptors</a> (TDD). </li>
    <li> 2016-03-01: Two papers are accepted by CVPR 2016. </li>
      <li> 2015-12-10: <alert>Our SIAT_MMLAB team secures the 2nd place for scene recognition at ILSVRC 2015 </alert> [ <a href='http://image-net.org/challenges/LSVRC/2015/results#scene'>Result</a> ].</li>
    <li> 2015-09-30: We rank 3rd for cultural event recognition on ChaLearn Looking at People challenge, at ICCV 2015. </li>
      <li> 2015-08-07: <alert>We release the Places205-VGGNet models</alert> [ <a href='https://github.com/wanglimin/Places205-VGGNet'>Link</a> ]. </li>
    <li> 2015-07-22: <alert>Code of Trajectory-Pooled Deep-onvolutional Descriptors (TDD) is released</alert> [ <a href='https://github.com/wanglimin/TDD'>Link</a> ]. </li>
		<li> 2015-07-15: <alert>Very deep two stream ConvNets are proposed for action recognition</alert> [ <a href='http://arxiv.org/abs/1507.02159'>Link</a> ]. </li>
	        <li> 2015-03-15: We are the 1st winner of both tracks for action recognition and cultural event recognition, on ChaLearn <a href='http://gesture.chalearn.org/'>Looking at People Challenge</a> at CVPR 2015. </li>
		 <li> 2015-03-03: One paper is accepted by CVPR 2015, details coming soon. </li>
	        <li> 2014-09-05: We rank 4th for action recognition and 2nd for action detection, on <a href='http://crcv.ucf.edu/THUMOS14/home.html'>THUMOS'14 Challenge</a> at ECCV 2014. </li>
		<li> 2014-06-16: Two papers are accepted by ECCV 2014. </li>
		<li> 2014-06-10: We are the 1st winner of both track 1 and track2, and rank 4th for track3, on ChaLearn <a href='http://gesture.chalearn.org/'>Looking at People Challenge</a> at ECCV 2014. </li>
		<li> 2014-05-20: A comprehensive study paper on action recognition [ <a href='http://arxiv.org/abs/1405.4506'>Link</a> ]. </li>
		<li> 2014-05-16: New homepage on Github launched! </li>
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Publications [ <a href='publication.html'>Full List</a> ] [ <a href='https://scholar.google.com.hk/citations?user=HEuN8PcAAAAJ&hl=en'>Google Scholar</a> ] [ <a href='https://github.com/MCG-NJU'>Github: MCG-NJU</a> ] </h2>

    <div class="paper" id="JointFormer"><img class="paper" src="papers/JointFormer_tpami25.PNG" title="JointFormer: A Unifed Framework with Joint Modeling for Video Object Segmentation" />
<div> <strong>JointFormer: A Unifed Framework with Joint Modeling for Video Object Segmentation</strong><br />
J. Zhang, Y. Cui, G. Wu, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2025. <br />
[ <a href='https://ieeexplore.ieee.org/document/10949703'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/JointFormer'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="PDPP"><img class="paper" src="papers/PDPP_tpami24.PNG" title="PDPP: Projected Diffusion for Procedure Planning in Instructional Videos" />
<div> <strong>PDPP: Projected Diffusion for Procedure Planning in Instructional Videos</strong><br />
H. Wang, Y. Wu, S. Guo, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2025. <br />
[ <a href='https://ieeexplore.ieee.org/document/10804102'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/PDPP'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="stmixer"><img class="paper" src="papers/stmixer_tpami24.PNG" title="STMixer: A One-Stage Sparse Action Detector" />
<div> <strong>STMixer: A One-Stage Sparse Action Detector</strong><br />
T. Wu, M. Cao, Z. Gao, G. Wu, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024. <br />
[ <a href='https://ieeexplore.ieee.org/document/10496238'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/STMixer'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="mixformer"><img class="paper" src="papers/mixformer_tpami24.PNG" title="MixFormer: End-to-End Tracking with Iterative Mixed Attention" />
<div> <strong>MixFormer: End-to-End Tracking with Iterative Mixed Attention</strong><br />
Y. Cui, C. Jiang, G. Wu, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2024. <br />
[ <a href='https://ieeexplore.ieee.org/document/10380715'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MixFormer'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="camliflow"><img class="paper" src="papers/camliflow_23.PNG" title="Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR Fusion" />
<div> <strong>Learning Optical Flow and Scene Flow with Bidirectional Camera-LiDAR Fusion</strong><br />
H.Liu, T. Lu, Y. Xu, J. Liu, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023. <br />
[ <a href='https://ieeexplore.ieee.org/document/10310261'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/CamLiFlow'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="dmdetr"><img class="paper" src="papers/dmdetr.PNG" title="Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding" />
<div> <strong>Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding</strong><br />
F. Shi, R. Gao, W. Huang, L. Wang <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023. <br />
[ <a href='https://ieeexplore.ieee.org/document/10298801'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/Dynamic-MDETR'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="POSE"><img class="paper" src="papers/pose_22.JPG" title="Recovering 3D Human Mesh from Monocular Images: A Survey" />
<div> <strong>Recovering 3D Human Mesh from Monocular Images: A Survey</strong><br />
Y. Tian, H. Zhang, Y. Liu, L. Wang<br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023. <br />
[ <a href='https://ieeexplore.ieee.org/document/10195242'>Paper</a> ] [ <a href='https://github.com/tinatiansjz/hmr-survey'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TP"><img class="paper" src="papers/TP_TPAMI23.PNG" title="Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection" />
<div> <strong>Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection</strong><br />
J. Tan, Y. Wang, G. Wu L. Wang<br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023. <br />
[ <a href='https://ieeexplore.ieee.org/document/10144649'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/TemporalPerceiver'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="pro_vp"><img class="paper" src="papers/pro_vp_ijcv_24.PNG" title="Progressive Visual Prompt Learning with Contrastive Feature Re-formation" />
<div> <strong>Progressive Visual Prompt Learning with Contrastive Feature Re-formation</strong><br />
C. Xu, Y. Zhu, H. Shen, B. Chen, Y. Liao, X. Chen, L. Wang <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2024. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-024-02172-x'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/ProVP'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

        <div class="paper" id="vlg"><img class="paper" src="papers/vlg_ijcv_24.PNG" title="VLG: General Video Recognition with Web Textual Knowledge" />
<div> <strong>VLG: General Video Recognition with Web Textual Knowledge</strong><br />
J. Lin, Z. Liu, W. Wang, W. Wu, L. Wang <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2024. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-024-02081-z'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/VLG'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

      <div class="paper" id="logn"><img class="paper" src="papers/logn_23.PNG" title="Logit Normalization for Long-tail Object Detection" />
<div> <strong>Logit Normalization for Long-tail Object Detection</strong><br />
L. Zhang, Y. Teng, L. Wang <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2024. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-023-01971-y'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/LogN'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

    <div class="paper" id="DGN"><img class="paper" src="papers/dgn_23.PNG" title="Dual Graph Networks for Pose Estimation in Crowded Scenes" />
<div> <strong>Dual Graph Networks for Pose Estimation in Crowded Scenes</strong><br />
J. Tu, G. Wu, L. Wang <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2024. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-023-01901-y'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/DGN'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="LIP"><img class="paper" src="papers/GaoWW_19.png" title="LIP: Local Importance-based Pooling" />
<div> <strong>LIP: Local Importance-based Pooling</strong><br />
Z. Gao, L. Wang, G. Wu <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2023. <br/>
[ <a href='https://link.springer.com/article/10.1007/s11263-022-01707-4'>Paper</a> ] [ <a href='https://github.com/sebgao/LIP'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="CDG"><img class="paper" src="papers/CDG_IJCV22.PNG" title="Cross-Domain Gated Learning for Domain Generalization" />
<div> <strong>Cross-Domain Gated Learning for Domain Generalization</strong><br />
  D. Du, J. Chen, Y. Li, K. Ma, G. Wu, Y Zheng, L. Wang <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2022. <br/>
[ <a href='https://link.springer.com/article/10.1007/s11263-022-01674-w'>Paper</a> ] [ <a href=''>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="CMPT"><img class="paper" src="papers/CMPT_IJCV21.PNG" title="Cross-Modal Pyramid Translation for RGB-D Scene Recognition" />
<div> <strong>Cross-Modal Pyramid Translation for RGB-D Scene Recognition</strong><br />
  D. Du, L. Wang, Z. Li, G. Wu <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2021. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-021-01475-7'>Paper</a> ]  [ <a href='https://github.com/MCG-NJU/CMPT'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="internvideo2"><img class="paper" src="papers/internvideo2_2024.PNG" title="InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" />
<div> <strong>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</strong><br />
Y. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, Y. Shi, T. Jiang, S. Li, H. Zhang, Y. Huang, Y. Qiao, Y. Wang, L. Wang <br />
in European Conference on Computer Vision (<strong>ECCV</strong>), 2024. <br />
[ <a href='https://arxiv.org/abs/2403.15377'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/InternVideo'>Code</a> ] <br />
<alert>STOA performance on more than 60 video understanding tasks.</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="videomaeV2"><img class="paper" src="papers/VideoMAEV2_CVPR23.PNG" title="VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking" />
<div> <strong>VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking</strong><br />
L. Wang, B. Huang. Z. Zhao, Z. Tong, Y. He, Y. Wang, Y. Wang, Yu Qiao<br />
in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <br />
[ <a href='https://arxiv.org/abs/2303.16727'>Paper</a> ] [ <a href='https://github.com/OpenGVLab/VideoMAEv2'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="videomae"><img class="paper" src="papers/videomae_22.JPG" title="VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training<" />
<div> <strong>VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</strong><br />
Z. Tong, Y. Song, J. Wang, L. Wang<br />
in Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022. <br />
[ <a href='https://arxiv.org/abs/2203.12602'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/VideoMAE'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="mixformer"><img class="paper" src="papers/mixformer_22.JPG" title="MixFormer: End-to-End Tracking with Iterative Mixed Attention" />
<div> <strong>MixFormer: End-to-End Tracking with Iterative Mixed Attention</strong><br />
Y. Cui, C. Jiang, L. Wang, G. Wu <br />
 in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. <br />
[ <a href='https://arxiv.org/abs/2203.11082'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MixFormer'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="adamixer"><img class="paper" src="papers/adamixer_22.JPG" title="AdaMixer: A Fast-Converging Query-Based Object Detector" />
<div> <strong>AdaMixer: A Fast-Converging Query-Based Object Detector</strong><br />
Z. Gao, L. Wang, B. Han, S. Guo <br />
 in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. <br />
[ <a href='https://arxiv.org/abs/2203.16507'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/AdaMixer'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="MultiSports"><img class="paper" src="papers/multisports_21.png" title="MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions" />
<div> <strong>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions</strong><br />
Y. Li, L. Chen, R. He, Z. Wang, G. Wu, L. Wang<br />
in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2021. <br />
[ <a href='https://arxiv.org/abs/2105.07404'>Paper</a> ] [ <a href='https://deeperaction.github.io/multisports/'>Data</a> ] [ <a href='https://github.com/MCG-NJU/MultiSports/'>Code</a> ] [ <a href='https://competitions.codalab.org/competitions/32066'>Challenge</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TDN"><img class="paper" src="papers/tdn_20.png" title="TDN: Temporal Difference Networks for Efficient Action Recognition" />
<div> <strong>TDN: Temporal Difference Networks for Efficient Action Recognition</strong><br />
L. Wang, Z. Tong, B. Ji, G. Wu<br />
in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021.  <br />
[ <a href='https://arxiv.org/abs/2012.10071'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/TDN'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="Zhao_ICCV17"><img class="paper" src="papers/Zhao_ICCV17.png" title="Temporal Action Detection with Structured Segment Networks" />
<div> <strong>Temporal Action Detection with Structured Segment Networks</strong><br />
Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin<br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2020. <br/>
[ <a href='https://link.springer.com/article/10.1007/s11263-019-01211-2'>Paper</a> ] [ <a href='https://github.com/yjxiong/action-detection'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="MOC-Dector"><img class="paper" src="papers/MOC_20.png" title="Actions as Moving Points" />
<div> <strong>Actions as Moving Points</strong><br />
Y. Li, Z. Wang, L. Wang, G. Wu <br />
in European Conference on Computer Vision (<strong>ECCV</strong>), 2020. <br />
[ <a href='https://arxiv.org/abs/2001.04608'>Paper</a> ] [ <a href='https://github.com/MCG-NJU/MOC-Detector'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TSN_TPAMI19"><img class="paper" src="papers/TSN_TPAMI19.PNG" title="" />
<div> <strong>Temporal Segment Networks for Action Recognition in Videos</strong><br />
L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool  <br />
in IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2019. <br/>
[ <a href='https://ieeexplore.ieee.org/document/8454294'>Paper</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ] <br />
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangWQV_IJCV18"><img class="paper" src="papers/WangWQV_IJCV18.png" title="Transferring Deep Object and Scene Representations for Event Recognition in Still Images" />
<div> <strong>Transferring Deep Object and Scene Representations for Event Recognition in Still Images</strong><br />
L. Wang, Z. Wang, Y. Qiao, and L. Van Gool <br />
in International Journal of Computer Vision (<strong>IJCV</strong>), 2018. <br />
[ <a href='https://link.springer.com/article/10.1007/s11263-017-1043-5'>Paper</a> ] [ <a href='https://github.com/wangzheallen/transfer_ijcv'>Code</a> ] <br />
<alert>STOA performance for event recognition on ChaLearn LAP cultural event, WIDER datasets.</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangLLV"><img class="paper" src="papers/WangLLV_18.png" title="Appearance-and-Relation Networks for Video Classification" />
<div> <strong>Appearance-and-Relation Networks for Video Classification</strong><br />
L. Wang, W. Li, W. Li, and L. Van Gool <br />
in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018. <br />
[ <a href='https://arxiv.org/abs/1711.09125'>Paper</a> ] [ <a href='https://github.com/wanglimin/ARTNet'>Code</a> ] <br />
<alert>A new architecture for spatiotemporal feature learning.</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangXLV_CVPR17"><img class="paper" src="papers/WangXLV_CVPR17.png" title="UntrimmedNets for Weakly Supervised Action Recognition and Detection" />
<div> <strong>UntrimmedNets for Weakly Supervised Action Recognition and Detection</strong><br />
L. Wang, Y. Xiong, D. Lin, and L. Van Gool <br />
in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. <br />
[ <a href='https://arxiv.org/abs/1703.03329'>Paper</a> ] [ <a href='papers/WangXLV_CVPR17.bib'>BibTex</a> ][ <a href='https://github.com/wanglimin/UntrimmedNet'>Code</a> ] <br />
<alert>An end-to-end architecture to learn from untrimmed videos.</alert>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="WangXWQLTV_ECCV16"><img class="paper" src="papers/WangXWQLTV_ECCV16.png" title="Temporal Segment Networks: Towards Good Practices for Deep Action Recognition" />
<div> <strong>Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</strong><br />
L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool  <br />
in European Conference on Computer Vision (<strong>ECCV</strong>), 2016. <br />
[ <a href='papers/WangXWQLTV_ECCV16.pdf'>Paper</a> ]  [ <a href='papers/WangXWQLTV_ECCV16.bib'>BibTex</a> ] [ <a href='papers/WangXWQLTV_ECCV16_Poster.pdf'>Poster</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ] [ <a href='https://ieeexplore.ieee.org/document/8454294'>Journal Version</a>]<br />
<alert>Proposing a segmental architecture and obtaining the state-of-the-art performance on UCF101 and HMDB51</alert>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="WangQT_CVPR15"><img class="paper" src="papers/WangQT_CVPR15.jpg" title="Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors" />
<div> <strong>Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</strong><br />
L. Wang, Y. Qiao, and X. Tang <br />
in IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2015. <br />
[ <a href='papers/WangQT_CVPR15.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR15.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR15_abstract.pdf'>Extended Abstract</a> ] [ <a href='papers/WangQT_CVPR15_Poster.pdf'>Poster<a> ] [ <a href = 'tdd/index.html'>Project Page</a> ] [ <a href = 'https://github.com/wanglimin/TDD'>Code</a> ] <br />
<alert>State-of-the-art performance: HMDB51: 65.9%, UCF101: 91.5%.</alert>
</div>
<div class="spanner"></div>
</div>
<!--
<div class="paper" id="WangWGQ_LAP15"><img class="paper" src="papers/WangWGQ_ChaLearnLAP15.jpg" title="Better Exploiting OS-CNNs for Better Event Recognition in Images" />
<div> <strong>Better Exploiting OS-CNNs for Better Event Recognition in Images</strong><br />
L. Wang, Z. Wang, S. Guo, and Y. Qiao <br />
ChaLearn Looking at People (<strong>LAP</strong>) workshop, <strong>ICCV</strong>, 2015. <br />
[ <a href='papers/WangWGQ_ChaLearnLAP15.pdf'>Paper</a>  ]  [ <a href='papers/WangWGQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='papers/WangWGQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ] <br />
<alert>Obtain 84.7% mAP and secure the 3rd place.</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangWDQ_LAP15"><img class="paper" src="papers/WangWDQ_ChaLearnLAP15.jpg" title="Event Recognition Using Object-Scene Convolutional Neural Networks" />
<div> <strong>Object-Scene Convolutional Neural Networks for Event Recognition in Images</strong><br />
L. Wang, Z. Wang, W. Du, and Y. Qiao <br />
ChaLearn Looking at People (<strong>LAP</strong>) workshop, <strong>CVPR</strong>, 2015. <br />
[ <a href='papers/WangWDQ_ChaLearnLAP15.pdf'>Paper</a>  ]  [ <a href='papers/WangWDQ_ChaLearnLAP15.bib'>BibTex</a> ] [ <a href='papers/WangWDQ_ChaLearnLAP15_slide.pdf'>Presentation</a> ] [ <a href = 'cultural_event/index.html'>Project Page</a> ] <br />
<alert>Obtain 85.5% mAP and rank 1st on the track of cultural event recognition.</alert>
</div>
<div class="spanner"></div>
</div>
>

<!--
<div class="paper" id="WangGHQ15"><img class="paper" src="papers/WangGHQ15.jpg" title="Places205-VGGNet Models for Scene Recognition" />
<div> <strong>Places205-VGGNet Models for Scene Recognition</strong><br />
L. Wang, S. Guo, W. Huang, and Y. Qiao <br />
ArXiv 1508.01667, 2015. <br />
[ <a href='papers/WangGHQ15.pdf'>Paper</a>  ]  [ <a href='papers/WangGHQ15.bib'>BibTex</a> ] [ <a href='https://github.com/wanglimin/Places205-VGGNet.git'>Project Page</a> ] <br />
<alert>Obtain the state-of-the-art performance on the datasets of Places205, SUN397, and MIT67</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangXWQ15"><img class="paper" src="papers/WangXWQ15.jpg" title="Towards Good Practices for Very Deep Two-Stream ConvNets" />
<div> <strong>Towards Good Practices for Very Deep Two-Stream ConvNets</strong><br />
L. Wang, Y. Xiong, Z. Wang, and Y. Qiao <br />
ArXiv 1507.02159, 2015. <br />
[ <a href='papers/WangXWQ15.pdf'>Paper</a>  ]  [ <a href='papers/WangXWQ15.bib'>BibTex</a> ] [ <a href='https://github.com/yjxiong/caffe/tree/action_recog'>Code</a> ] <br />
<alert>Obtain 91.4% accuracy on the UCF101 dataset. Caffe extension for Multi-GPU training released.</alert>
</div>
<div class="spanner"></div>
</div>
>

<!--
<div class="paper" id="WangQT_ECCV14"><img class="paper" src="papers/WangQT_ECCV14.jpg" title="Video Action Detection with Relational Dynamic-Poselets" />
<div> <strong>Video Action Detection with Relational Dynamic-Poselets</strong><br />
L. Wang, Y. Qiao, and X. Tang <br />
European Conference on Computer Vision (<strong>ECCV</strong>), 2014. <br />
[ <a href='papers/WangQT_ECCV14.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ECCV14.bib'>BibTex</a> ] [ <a href='papers/WangQT_ECCV14_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ECCV14_Spotlight.wmv'>Spotlight</a> ] [ <a href='release_plot.rar'>Code</a> ] <br />
<alert>Joint action recognition, action detection, and pose estimation.</alert>
</div>
<div class="spanner"></div>
</div>
-->

<!--
<div class="paper" id="WangQT_TIP14"><img class="paper" src="papers/WangQT_TIP14.jpg" title="Latent Hierarchical Model of Temporal Structure for Complex Activity Classification" />
<div> <strong>Latent Hierarchical Model of Temporal Structure for Complex Activity Classification</strong><br />
L. Wang, Y. Qiao, and X. Tang <br />
IEEE Transactions on Image Processing (<strong>TIP</strong>), Vol. 23, No. 2, 2014. <br />
[ <a href='papers/WangQT_TIP14.pdf'>Paper</a> ]  [ <a href='papers/WangQT_TIP14.bib'>BibTex</a> ]
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangQT_ICCV13"><img class="paper" src="papers/WangQT_ICCV13.jpg" title="Mining Motion Atoms and Phrases for Complex Action Recognition" />
<div> <strong>Mining Motion Atoms and Phrases for Complex Action Recognition</strong><br />
L. Wang, Y. Qiao, and X. Tang <br />
IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2013. <br />
[ <a href='papers/WangQT_ICCV13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_ICCV13.bib'>BibTex</a> ] [ <a href='papers/WangQT_ICCV13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_ICCV13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'mofap/index.html'>Project Page</a> ]
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangQT_CVPR13"><img class="paper" src="papers/WangQT_CVPR13.jpg" title="Motionlets: Mid-Level 3D Parts for Human Motion Recognition" />
<div> <strong>Motionlets: Mid-Level 3D Parts for Human Motion Recognition</strong><br />
L. Wang, Y. Qiao, and X. Tang <br />
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2013. <br />
[ <a href='papers/WangQT_CVPR13.pdf'>Paper</a> ]  [ <a href='papers/WangQT_CVPR13.bib'>BibTex</a> ] [ <a href='papers/WangQT_CVPR13_Poster.pdf'>Poster</a> ] [ <a href='papers/WangQT_CVPR13_Spotlight.ppt'>Spotlight</a> ] [ <a href = 'motionlet/index.html'>Project Page</a> ]
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="WangWLC_MM11"><img class="paper" src="papers/WangWLC_MM11.jpg" title="Multiclass Object Detection by Combining Local Appearances and Context" />
<div> <strong>Multiclass Object Detection by Combining Local Appearances and Context</strong><br />
L. Wang, Y. Wu, T. Lu, and K. Chen<br />
in ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2011. <br />
[ <a href='papers/WangWLC_MM11.pdf'>Paper</a> ] [ <a href='papers/WangWLC_MM11.bib'>BibTex</a> ]
</div>
<div class="spanner"></div>
</div>

-->
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<li><alert>ActivityNet Large Scale Activity Recognition Challenge, 2016</alert>: Untrimmed Video Classification, <strong>Rank</strong>: 1/24. </li>
<li><alert>ImageNet Large Scale Visual Recognition Challenge, 2015</alert>: Scene Recognition, <strong>Rank</strong>: 2/25. </li>
<li><strong>ChaLearn Looking at People Challenge, 2015</strong>, <strong>Rank</strong>: 1/6</li>
<li><strong>THUMOS Action Recognition Challenge, 2015</strong>, <strong>Rank</strong>: 5/11.</li>
<li><strong>ChaLearn Looking at People Challenge, 2014</strong> , <strong>Rank</strong>: 1/6, 4/17.</li>
<li><strong>THUMOS Action Recognition Challenge, 2014</strong>,  <strong>Rank</strong>: 4/14, 2/3.</li>
<li><strong>ChaLearn Multi-Modal Gesture Recognition Challenge, 2013 </strong>,  <strong>Rank</strong>: 4/54.</li>
<li><strong>THUMOS Action Recognition Challenge, 2013</strong>, <strong>Rank</strong>: 4/16.</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Academic Service</h2>
<div class="paper">
<strong>Journal Reviewer</strong> <br>
<p> IEEE Transactions on Pattern Analysis and Machine Intelligence </p>
<p> IEEE Transactions on Image Processing </p>
<p> IEEE Transactions on Multimedia </p>
<p> IEEE Transactions on Circuits and Systems for Video Technology </p>
<p> Pattern Recognition </p>
<p> Pattern Recognition Letter </p>
<p> Image and Vision Computing </p>
<p> Computer Vision and Image Understanding </p>
<br />
<strong>Conference Reviewer</strong> <br>
<p> IEEE Conference on Computer Vision and Pattern Recognition, 2017 </p>
<p> IEEE International Conference on Automatic Face and Gesture Recognition, 2017 </p>
<p> European Conference on Computer Vision, 2016 </p>
<p> Asian Conference on Computer Vision, 2016 </p>
<p> International Conference on Pattern Recognition, 2016 </p>
<div class="spanner"></div>
</div>
</div>
</div>

<!--
<div style="clear: both;">
<div class="section"><h2>Courses</h2>
<div class="paper">
<strong>Teaching Assistant for</strong> <br>
<p> ENGG2430, Probability and Statistics for Engineers ( Spring 2015 ). </p>
<p> ENGG1100, Introduction to Engineering Design ( Autumn 2014 ). </p>
<br />
<strong>Graduate Courses</strong> <br>
<p> SEEM 5121, Numerical Optimization ( Spring 2015 ) [ <a href='http://www1.se.cuhk.edu.hk/~sqma/SEEM5121'>Webpage</a> ]. </p>
<p> IERG 6130, Advanced Topics on Machine Learning and Probabilistic Inference ( Spring 2015 ) [ <a href='http://lindahua.github.io/MLPI/'>Webpage</a> ]. </p>
<p> ELEG 5481, Signal Processing Optimization Techniques ( Spring 2013 ) [ <a href='http://dsp.ee.cuhk.edu.hk/eleg5481/'>Webpage</a> ]. </p>
<p> IERG 6210, Advanced Topic in Information Processing (Spring 2012 ) [ <a href='https://course.ie.cuhk.edu.hk/~ierg6210/'>Webpage</a> ]. </p>
<p> CSCI 5160, Spectral Algorithm ( Spring 2012) [ <a href='http://www.cse.cuhk.edu.hk/~chi/csc5160/'>Webpage</a> ]. </p>
<p> SEEM 5520, Optimization I ( Autumn 2011 ) [ <a href='http://www1.se.cuhk.edu.hk/~manchoso/1112/seem5520/'>Webpage</a> ]. </p>
<p> IERG 5154, Information Theory ( Autumn 2011 ) [ <a href="http://ieg5154.pbworks.com/w/page/8690729/FrontPage">Webpage</a> ]. </p>
</div>
</div>
</div>
-->

<div style="clear: both;">
<div class="section"><h2>Friends</h2>
<div class="paper">
<a href='http://www.vision.ee.ethz.ch/~liwenw/'>Wen Li</a> (ETH), <a href='https://ait.ethz.ch/people/song/'>Jie Song</a> (ETH), <a href='http://guoshengcv.github.io/'>Sheng Guo</a> (Malong), <a href='http://www.whuang.org/'>Weilin Huang</a> (Malong), <a href='http://zbwglory.github.io/'>Bowen Zhang</a> (USC), <a href='http://wangzheallen.github.io/'>Zhe Wang</a> (UCI), <a href='http://liwei.ml/'>Wei Li</a> (Google), <a href='http://personal.ie.cuhk.edu.hk/~xy012/'>Yuanjun Xiong</a> (Amazon), <a href='http://pengxj.github.io/'>Xiaojiang Peng</a> (SIAT), <a href='https://zhuoweic.github.io/'>Zhuowei Cai</a> (Google), <a href='https://scholar.google.com.sg/citations?user=fPwq28oAAAAJ&hl=en'>Xingxing Wang</a> (NTU)
</div>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 15th June., 2023</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>
</body>
</html>
